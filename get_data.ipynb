{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6c7bbc",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48fcd7a0",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas_datareader.data as web\n",
    "import polars as pl\n",
    "\n",
    "from functions.utils.func import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3c3c98",
   "metadata": {},
   "source": [
    "# CRSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded06e95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T19:25:03.652284900Z",
     "start_time": "2023-08-01T19:25:03.636039800Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_crsp():\n",
    "    # Read in crsp dataset\n",
    "    crsp = pd.read_parquet(get_load_data_large_dir() / 'crsp.parquet.brotli')\n",
    "    \n",
    "    # Rename Columns\n",
    "    print('Rename columns')\n",
    "    crsp.columns = crsp.columns.str.lower()\n",
    "    crsp = crsp.rename(columns={'prc':'Close', 'bid':'High', 'ask':'Low', 'openprc':'Open', 'shrout':'outstanding', 'vol':'Volume', 'cfacpr':'adj_price'})\n",
    "    \n",
    "    # Adjust closing price\n",
    "    print('Adjusting Close Price')\n",
    "    crsp['Close'] = crsp['Close']/crsp['adj_price']\n",
    "\n",
    "    # Set and sort index\n",
    "    print('Set and sort indices')\n",
    "    crsp.date = pd.to_datetime(crsp.date)\n",
    "    crsp = crsp.set_index(['permno', 'date'])\n",
    "    crsp = crsp.sort_index(level=['permno', 'date'])\n",
    "    \n",
    "    # Remove duplicate indices and replace all infinity with nan\n",
    "    print('Remove duplicate indices and infinity')\n",
    "    crsp = crsp[~crsp.index.duplicated(keep='first')]\n",
    "    crsp = crsp.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Remove stocks that have more than 1 NAN values in their Closing price column\n",
    "    # Stocks that get delisted have 1 row of NAN values as their last row\n",
    "    # Stocks that switch ticker (WM to COOP: 81593) have rows of NAN valuescap = cap.dropna(subset='Close')\n",
    "    # Afterwards, drop all rows that have NAN values in Close (every delisted permno stock only has 1 NAN in Close now)\n",
    "    print('Remove stocks with NAN')\n",
    "    nan_counts = crsp.groupby('permno')['Close'].apply(lambda x: x.isna().sum())\n",
    "    valid_permnos = nan_counts[nan_counts <= 1].index.tolist()\n",
    "    crsp = crsp[crsp.index.get_level_values('permno').isin(valid_permnos)]\n",
    "    crsp = crsp.dropna(subset='Close')\n",
    "    \n",
    "    # Remove dates in stocks that have a negative closing price\n",
    "    crsp = crsp[crsp['Close'] >= 0]\n",
    "    \n",
    "    # Remove stocks that do not have at least 3 years worth of year data\n",
    "    print('Set length to 3 years')\n",
    "    crsp = set_length(crsp, 3)\n",
    "    \n",
    "    # Drop tickers that do not have over 6B market cap\n",
    "    crsp['market_cap'] = crsp['Close'] * crsp['outstanding'] * 1000\n",
    "    avg_cap = crsp.groupby('permno')['market_cap'].mean()\n",
    "    above_cap = avg_cap[avg_cap > 6_000_000_000].index\n",
    "    crsp = crsp[crsp.index.get_level_values('permno').isin(above_cap)]\n",
    "\n",
    "    # Export list of stocks\n",
    "    print('Exporting stock list')\n",
    "    print(f'Number of stocks: {len(get_stock_idx(crsp))}')\n",
    "    export_stock(crsp, get_load_data_large_dir() / 'permno_to_train.csv')\n",
    "\n",
    "    # Export crsp data\n",
    "    print('Exporting Crsp')\n",
    "    crsp.to_parquet(get_load_data_parquet_dir() / 'data_crsp.parquet.brotli', compression='brotli')\n",
    "    \n",
    "    # Export ohclv\n",
    "    print('Exporting Price')\n",
    "    ohclv = crsp[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "    ohclv = ohclv.astype(float)\n",
    "    ohclv.to_parquet(get_load_data_parquet_dir() / 'data_price.parquet.brotli', compression='brotli')\n",
    "    \n",
    "    # Export date\n",
    "    print('Export Date')\n",
    "    date = crsp.drop(columns=crsp.columns)\n",
    "    date.to_parquet(get_load_data_parquet_dir() / 'data_date.parquet.brotli', compression='brotli')\n",
    "\n",
    "    # Export ticker\n",
    "    print('Exporting Tickers')\n",
    "    ticker = crsp[['ticker']]\n",
    "    ticker.to_parquet(get_load_data_parquet_dir() / 'data_ticker.parquet.brotli', compression='brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d7ffe6c9457314",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_crsp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df0e660",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohclv = pd.read_parquet(get_load_data_parquet_dir() / 'data_price.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757548b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = pd.read_parquet(get_load_data_parquet_dir() / 'data_ticker.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531f19b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = pd.read_parquet(get_load_data_parquet_dir() / 'data_date.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a95e3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = read_stock(get_load_data_large_dir() / 'permno_to_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b23028-05d4-472f-a767-45794cd87ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "crsp = pd.read_parquet(get_load_data_parquet_dir() / 'data_crsp.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f926956-6ae1-4a4c-930f-caed9e49f2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "crsp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3961e187-4e97-4e79-892b-409be339fd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohclv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697b32b3-3a6a-4648-8292-b5fde96bc75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stock)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b5df6f-1944-4314-a7b9-3970dd0c37c3",
   "metadata": {},
   "source": [
    "# CRSP Compustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0acd093-802d-411c-93d4-b4b4453399bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_crsp_compustat():\n",
    "    # Read in crsp_compustat\n",
    "    crsp_compustat = pd.read_parquet(get_load_data_large_dir() / 'crsp_compustat.parquet.brotli')\n",
    "    stock = read_stock(get_load_data_large_dir() / 'permno_to_train.csv')\n",
    "    \n",
    "    # Rename Columns\n",
    "    print('Rename columns')\n",
    "    crsp_compustat.columns = crsp_compustat.columns.str.lower()\n",
    "    crsp_compustat = crsp_compustat.rename(columns={'lpermno':'permno', 'tic':'ticker', 'datadate':'date', 'prccd':'Close', 'prchd':'High', 'prcld':'Low', 'prcod':'Open',\n",
    "                                                    'cshoc':'outstanding', 'cshtrd':'Volume', 'gsubind':'Subindustry', 'gind':'Industry', 'naics':'IndNAIC'})\n",
    "    \n",
    "    # Set and sort index\n",
    "    print('Set and sort indices')\n",
    "    crsp_compustat.date = pd.to_datetime(crsp_compustat.date)\n",
    "    crsp_compustat = crsp_compustat.set_index(['permno', 'date'])\n",
    "    crsp_compustat = crsp_compustat.sort_index(level=['permno', 'date'])\n",
    "    crsp_compustat = get_stocks_data(crsp_compustat, stock)\n",
    "    \n",
    "    # Remove duplicate indices and replace all infinity with nan\n",
    "    print('Remove duplicate indices and infinity')\n",
    "    crsp_compustat = crsp_compustat[~crsp_compustat.index.duplicated(keep='first')]\n",
    "    crsp_compustat = crsp_compustat.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # Fill NAN values for industries with -1\n",
    "    print('Fill industries with -1')\n",
    "    cols_to_fill = ['Industry', 'Subindustry', 'IndNAIC']\n",
    "    crsp_compustat[cols_to_fill] = crsp_compustat[cols_to_fill].fillna(-1)\n",
    "\n",
    "    # Export list of stocks\n",
    "    print('Exporting stock list')\n",
    "    print(f'Number of stocks: {len(get_stock_idx(crsp_compustat))}')\n",
    "    export_stock(crsp_compustat, get_load_data_large_dir() / 'permno_to_train_comp.csv')\n",
    "    stock_comp = read_stock(get_load_data_large_dir() / 'permno_to_train_comp.csv')\n",
    "\n",
    "    # Export crsp_compustat data\n",
    "    print('Exporting Crsp Compustat')\n",
    "    crsp_compustat.to_parquet(get_load_data_parquet_dir() / 'data_crsp_compustat.parquet.brotli', compression='brotli')\n",
    "    \n",
    "    # Export ind\n",
    "    print('Exporting Industries')\n",
    "    ind = crsp_compustat[['Industry']]\n",
    "    ind = ind.astype(int)\n",
    "    sub_ind = crsp_compustat[['Subindustry']]\n",
    "    sub_ind = sub_ind.astype(int)\n",
    "    ind_naic = crsp_compustat[['IndNAIC']]\n",
    "    ind_naic = ind_naic.astype(int)\n",
    "\n",
    "    # Merge ind data with crsp dataset\n",
    "    date = pd.read_parquet(get_load_data_parquet_dir() / 'data_date.parquet.brotli')\n",
    "    date = get_stocks_data(date, stock_comp)\n",
    "    ind = pd.merge(date, ind, left_index=True, right_index=True, how='left')\n",
    "    sub_ind = pd.merge(date, sub_ind, left_index=True, right_index=True, how='left')\n",
    "    ind_naic = pd.merge(date, ind_naic, left_index=True, right_index=True, how='left')\n",
    "    ind.to_parquet(get_load_data_parquet_dir() / 'data_ind.parquet.brotli', compression='brotli')\n",
    "    sub_ind.to_parquet(get_load_data_parquet_dir() / 'data_ind_sub.parquet.brotli', compression='brotli')\n",
    "    ind_naic.to_parquet(get_load_data_parquet_dir() / 'data_ind_naic.parquet.brotli', compression='brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f2df1a-b833-4a1c-9870-ffea4a39fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_crsp_compustat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d701047a-2d9f-4f48-8191-e1b7841e44d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = pd.read_parquet(get_load_data_parquet_dir() / 'data_ind.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36b694c-7ac5-47de-8e50-1864bdd1ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ind = pd.read_parquet(get_load_data_parquet_dir() / 'data_ind_sub.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0a6427-8174-40ac-af29-a1d790b7a28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "crsp_compustat = pd.read_parquet(get_load_data_parquet_dir() / 'data_crsp_compustat.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285c306f-1921-4908-a9a6-f3d3f32cca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_comp = read_stock(get_load_data_large_dir() / 'permno_to_train_comp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be721b6a-b7d6-4abe-8ae6-ecaa81fc53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baf1d22-ff2b-474f-91c3-56fb23e58f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stocks_data(ohclv, stock_comp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10dec0e-c2d1-4332-904d-7fbfbeb02116",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stock_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30daaf6",
   "metadata": {},
   "source": [
    "# Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72f9b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ind():\n",
    "    # Assign Fama industries based off given ranges\n",
    "    def assign_ind(df, column_name, sic_ranges, label):\n",
    "        # Sic from CRSP and Compustat\n",
    "        df['sic_temp_crsp'] = df['sic_crsp']\n",
    "        df['sic_temp_comp'] = df['sic_comp']\n",
    "\n",
    "        # Iterate through each range and assign industry\n",
    "        for r in sic_ranges:\n",
    "            if isinstance(r, tuple):\n",
    "                df.loc[(df['sic_temp_crsp'] >= r[0]) & (df['sic_temp_crsp'] <= r[1]), f'{column_name}_crsp'] = label\n",
    "                df.loc[(df['sic_temp_comp'] >= r[0]) & (df['sic_temp_comp'] <= r[1]), f'{column_name}_comp'] = label\n",
    "            else:\n",
    "                df.loc[df['sic_temp_crsp'] == r, f'{column_name}_crsp'] = label\n",
    "                df.loc[df['sic_temp_comp'] == r, f'{column_name}_comp'] = label\n",
    "        \n",
    "        df = df.drop(columns=['sic_temp_crsp', 'sic_temp_comp'])\n",
    "        return df\n",
    "\n",
    "    # Read in CRSP sic\n",
    "    crsp_sic = pd.read_parquet(get_load_data_parquet_dir() / 'data_crsp.parquet.brotli', columns=['siccd'])\n",
    "    crsp_sic = crsp_sic.rename(columns={'siccd':'sic_crsp'})\n",
    "    print('Finished crsp')\n",
    "\n",
    "    # Read in Compustat sic \n",
    "    comp_sic = pd.read_parquet(get_load_data_parquet_dir() / 'data_crsp_compustat.parquet.brotli', columns=['sic'])\n",
    "    comp_sic = comp_sic.rename(columns={'sic':'sic_comp'})\n",
    "    print('Finished compustat')\n",
    "    \n",
    "    # Merge CRSP and Compustat\n",
    "    combined = pd.merge(crsp_sic, comp_sic, left_index=True, right_index=True, how='left')\n",
    "    combined = combined[~combined.index.duplicated(keep='first')]\n",
    "    stock_comp = read_stock(get_load_data_large_dir() / 'permno_to_train_comp.csv')\n",
    "    combined = get_stock_data(combined, stock_comp)\n",
    "\n",
    "    # FF49 Industry ranges\n",
    "    fama_ind = {\n",
    "    'agric': [(100, 199), (200, 299), (700, 799), (910, 919), 2048],\n",
    "    'food': [(2000, 2009), (2010, 2019), (2020, 2029), (2030, 2039), (2040, 2046), (2050, 2059), (2060, 2063), (2070, 2079), (2090, 2092), 2095, (2098, 2099)],\n",
    "    'soda': [(2064, 2068), 2086, 2087, 2096, 2097],\n",
    "    'beer': [2080, 2082, 2083, 2084, 2085],\n",
    "    'smoke': [(2100, 2199)],\n",
    "    'toys': [(920, 999), (3650, 3651), 3652, 3732, (3930, 3931), (3940, 3949)],\n",
    "    'fun': [(7800, 7829), (7830, 7833), (7840, 7841), 7900, (7910, 7911), (7920, 7929), (7930, 7933), (7940, 7949), 7980, (7990, 7999)],\n",
    "    'books': [(2700, 2709), (2710, 2719), (2720, 2729), (2730, 2739), (2740, 2749), (2770, 2771), (2780, 2789), (2790, 2799)],\n",
    "    'hshld': [2047, (2391, 2392), (2510, 2519), (2590, 2599), (2840, 2843), 2844, (3160, 3161), (3170, 3171), 3172, (3190, 3199), 3229, 3260, (3262, 3263), 3269, (3230, 3231), (3630, 3639), (3750, 3751), 3800, (3860, 3861), (3870, 3873), (3910, 3911), 3914, 3915, (3960, 3962), 3991, 3995],\n",
    "    'clths': [(2300, 2390), (3020, 3021), (3100, 3111), (3130, 3131), (3140, 3149), (3150, 3151), (3963, 3965)],\n",
    "    'hlth': [(8000, 8099)],\n",
    "    'medeq': [3693, (3840, 3849), (3850, 3851)],\n",
    "    'drugs': [2830, 2831, 2833, 2834, 2835, 2836],\n",
    "    'chems': [(2800, 2809), (2810, 2819), (2820, 2829), (2850, 2859), (2860, 2869), (2870, 2879), (2890, 2899)],\n",
    "    'rubbr': [3031, 3041, (3050, 3053), (3060, 3069), (3070, 3079), (3080, 3089), (3090, 3099)],\n",
    "    'txtls': [(2200, 2269), (2270, 2279), (2280, 2284), (2290, 2295), 2297, 2298, 2299, (2393, 2395), (2397, 2399)],\n",
    "    'bldmt': [(800, 899), (2400, 2439), (2450, 2459), (2490, 2499), (2660, 2661), (2950, 2952), 3200, (3210, 3211), (3240, 3241), (3250, 3259), 3261, 3264, (3270, 3275), (3280, 3281), (3290, 3293), (3295, 3299), (3420, 3429), (3430, 3433), (3440, 3441), 3442, 3446, 3448, 3449, (3450, 3451), 3452, (3490, 3499), 3996],\n",
    "    'cnstr': [(1500, 1511), (1520, 1529), (1530, 1539), (1540, 1549), (1600, 1699), (1700, 1799)],\n",
    "    'steel': [3300, (3310, 3317), (3320, 3325), (3330, 3339), (3340, 3341), (3350, 3357), (3360, 3369), (3370, 3379), (3390, 3399)],\n",
    "    'fabpr': [3400, 3443, 3444, (3460, 3469), (3470, 3479)],\n",
    "    'mach': [(3510, 3519), (3520, 3529), 3530, 3531, 3532, 3533, 3534, 3535, 3536, 3538, (3540, 3549), (3550, 3559), (3560, 3569), 3580, 3581, 3582, 3585, 3586, 3589, (3590, 3599)],\n",
    "    'elceq': [3600, (3610, 3613), (3620, 3621), (3623, 3629), (3640, 3644), 3645, 3646, (3648, 3649), 3660, 3690, (3691, 3692), 3699],\n",
    "    'autos': [2296, 2396, (3010, 3011), 3537, 3647, 3694, 3700, 3710, 3711, 3713, 3714, 3715, 3716, 3792, (3790, 3791), 3799],\n",
    "    'aero': [3720, 3721, (3723, 3724), 3725, (3728, 3729)],\n",
    "    'ships': [(3730, 3731), (3740, 3743)],\n",
    "    'guns': [(3760, 3769), 3795, (3480, 3489)],\n",
    "    'gold': [(1040, 1049)],\n",
    "    'mines': [(1000, 1009), (1010, 1019), (1020, 1029), (1030, 1039), (1050, 1059), (1060, 1069), (1070, 1079), (1080, 1089), (1090, 1099), (1100, 1119), (1400, 1499)],\n",
    "    'coal': [(1200, 1299)],\n",
    "    'oil': [1300, (1310, 1319), (1320, 1329), (1330, 1339), (1370, 1379), 1380, 1381, 1382, 1389, (2900, 2912), (2990, 2999)],\n",
    "    'util': [4900, (4910, 4911), (4920, 4922), 4923, (4924, 4925), (4930, 4931), 4932, 4939, (4940, 4942)],\n",
    "    'telcm': [4800, (4810, 4813), (4820, 4822), (4830, 4839), (4840, 4841), 4880, 4890, 4891, 4892, 4899],\n",
    "    'persv': [(7020, 7021), (7030, 7033), 7200, (7210, 7212), 7214, (7215, 7216), 7217, 7219, (7220, 7221), (7230, 7231), (7240, 7241), (7250, 7251), (7260, 7269), (7270, 7290), 7291, (7292, 7299), 7395, 7500, (7520, 7529), (7530, 7539), (7540, 7549), 7600, 7620, 7622, 7623, 7629, 7630, 7640, (7690, 7699), (8100, 8199), (8200, 8299), (8300, 8399), (8400, 8499), (8600, 8699), (8800, 8899), (7510, 7515)],\n",
    "    'bussv': [(2750, 2759), 3993, 7218, 7300, (7310, 7319), (7320, 7329), (7330, 7339), (7340, 7342), 7349, (7350, 7351), 7352, 7353, 7359, (7360, 7369), 7374, 7376, 7377, 7378, 7379, 7380, (7381, 7382), 7383, 7384, 7385, 7389, 7390, 7391, (7392, 7392), 7393, 7394, 7396, 7397, 7399, (7519, 7519), 8700, (8710, 8713), (8720, 8721), (8730, 8734), (8740, 8748), (8900, 8910), 8911, (8920, 8999), (4220, 4229)],\n",
    "    'hardw': [(3570, 3579), 3680, 3681, 3682, 3683, 3684, 3685, 3686, 3687, 3688, 3689, 3695],\n",
    "    'softw': [(7370, 7372), 7375, 7373],\n",
    "    'chips': [3622, 3661, (3662, 3662), 3663, 3664, 3665, 3666, 3669, (3670, 3679), (3810, 3810), (3812, 3812)],\n",
    "    'labeq': [3811, 3820, 3821, 3822, 3823, 3824, 3825, 3826, 3827, 3829, (3830, 3839)],\n",
    "    'paper': [(2520, 2549), (2600, 2639), (2670, 2699), (2760, 2761), (3950, 3955)],\n",
    "    'boxes': [(2440, 2449), (2640, 2659), (3220, 3221), (3410, 3412)],\n",
    "    'trans': [(4000, 4013), (4040, 4049), 4100, (4110, 4119), (4120, 4121), (4130, 4131), (4140, 4142), (4150, 4151), (4170, 4173), (4190, 4199), 4200, (4210, 4219), (4230, 4231), (4240, 4249), (4400, 4499), (4500, 4599), (4600, 4699), 4700, (4710, 4712), (4720, 4729), (4730, 4739), (4740, 4749), 4780, 4782, 4783, 4784, 4785, 4789],\n",
    "    'whlsl': [5000, (5010, 5015), (5020, 5023), (5030, 5039), (5040, 5042), 5043, 5044, 5045, 5046, 5047, 5048, 5049, (5050, 5059), 5060, 5063, 5064, 5065, (5070, 5078), 5080, 5081, 5082, 5083, 5084, 5085, (5086, 5087), 5088, 5090, (5091, 5092), 5093, 5094, 5099, 5100, (5110, 5113), (5120, 5122), (5130, 5139), (5140, 5149), (5150, 5159), (5160, 5169), (5170, 5172), (5180, 5182), (5190, 5199)],\n",
    "    'rtail': [5200, (5210, 5219), (5220, 5229), (5230, 5231), (5250, 5251), (5260, 5261), (5270, 5271), 5300, 5310, 5320, (5330, 5331), 5334, (5340, 5349), (5390, 5399), 5400, (5410, 5411), 5412, (5420, 5429), (5430, 5439), (5440, 5449), (5450, 5459), (5460, 5469), (5490, 5499), 5500, (5510, 5529), (5530, 5539), (5540, 5549), (5550, 5559), (5560, 5569), (5570, 5579), (5590, 5599), (5600, 5699), 5700, (5710, 5719), (5720, 5722), (5730, 5733), 5734, 5735, 5736, (5750, 5799), 5900, (5910, 5912), (5920, 5929), (5930, 5932), 5940, 5941, 5942, 5943, 5944, 5945, 5946, 5947, 5948, 5949, (5950, 5959), (5960, 5969), (5970, 5979), (5980, 5989), 5990, 5992, 5993, 5994, 5995, 5999],\n",
    "    'meals': [(5800, 5819), (5820, 5829), (5890, 5899), 7000, (7010, 7019), (7040, 7049), 7213],\n",
    "    'banks': [6000, (6010, 6019), 6020, 6021, 6022, 6023, 6025, 6026, 6027, (6028, 6029), (6030, 6036), (6040, 6059), (6060, 6062), (6080, 6082), (6090, 6099), 6100, (6110, 6111), (6112, 6113), (6120, 6129), (6130, 6139), (6140, 6149), (6150, 6159), (6160, 6169), (6170, 6179), (6190, 6199)],\n",
    "    'insur': [6300, (6310, 6319), (6320, 6329), (6330, 6331), (6350, 6351), (6360, 6361), (6370, 6379), (6390, 6399), (6400, 6411)],\n",
    "    'rlest': [6500, 6510, 6512, 6513, 6514, 6515, (6517, 6519), (6520, 6529), (6530, 6531), 6532, (6540, 6541), (6550, 6553), (6590, 6599), (6610, 6611)],\n",
    "    'fin': [(6200, 6299), 6700, (6710, 6719), (6720, 6722), 6723, 6724, 6725, 6726, (6730, 6733), (6740, 6779), 6790, 6791, 6792, 6793, 6794, 6795, 6798, 6799],\n",
    "    'other': [(4950, 4959), (4960, 4961), (4970, 4971), (4990, 4991)]\n",
    "    }\n",
    "\n",
    "    # Iterate through each key\n",
    "    for name, ranges in fama_ind.items():\n",
    "        print('-'*60)\n",
    "        print(name)\n",
    "        combined = assign_ind(combined, 'IndustryFama', ranges, name)\n",
    "\n",
    "    # Assign industry based off Compustat. If Compustat is NAN, then use CRSP\n",
    "    combined['IndustryFama'] = combined['IndustryFama_comp'].combine_first(combined['IndustryFama_crsp'])\n",
    "    combined['IndustryFama'], category_mapping = combined['IndustryFama'].factorize()\n",
    "    print(combined)\n",
    "    combined = combined[['IndustryFama']]\n",
    "\n",
    "    # Export FF49 Industries\n",
    "    combined.to_parquet(get_load_data_parquet_dir() / 'data_ind_fama.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cfdeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_ind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8045ecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_fama = pd.read_parquet(get_load_data_parquet_dir() / 'data_ind_fama.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44ba29e-643e-4654-9f9b-44de6eb3ee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_fama.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e590d2b",
   "metadata": {},
   "source": [
    "# Fama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a6b51e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-03T19:21:56.862578200Z",
     "start_time": "2023-08-03T19:21:56.843341Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_fama():\n",
    "    # Get Fama data from web\n",
    "    fama_data = (web.DataReader('F-F_Research_Data_5_Factors_2x3_daily', 'famafrench', start=2005)[0].rename(columns={'Mkt-RF': 'MARKET'}))\n",
    "    fama_data.index.names = ['date']\n",
    "    fama_data = fama_data.astype(float)\n",
    "    fama_data.to_parquet(get_load_data_parquet_dir() / 'data_fama.parquet.brotli', compression='brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3da433256bf0c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-03T19:21:59.133594400Z",
     "start_time": "2023-08-03T19:21:57.469391700Z"
    }
   },
   "outputs": [],
   "source": [
    "create_fama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54bf078bed94a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "fama = pd.read_parquet(get_load_data_parquet_dir() / 'data_fama.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abfb509-bde3-4b70-a708-e202ddfbe90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fama.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb11edc",
   "metadata": {},
   "source": [
    "# Macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41579ea4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T19:34:34.389161700Z",
     "start_time": "2023-08-15T19:34:34.285161400Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_macro():\n",
    "    # Read in IF\n",
    "    IF = pd.read_csv(get_load_data_large_dir() / 'macro' / 'fiveYearIR.csv')\n",
    "    IF.columns = ['date', '5YIF']\n",
    "    IF = IF.set_index(pd.to_datetime(IF['date'])).drop('date', axis=1)\n",
    "    IF = IF[~IF.index.duplicated(keep='first')]\n",
    "\n",
    "    # Read in medianCPI\n",
    "    medianCPI = pd.read_csv(get_load_data_large_dir() / 'macro' / 'medianCPI.csv')\n",
    "    medianCPI.columns = ['date', 'medCPI']\n",
    "    medianCPI['date'] = pd.to_datetime(medianCPI['date'])\n",
    "    medianCPI['date'] = (medianCPI['date'] + pd.DateOffset(months=1))\n",
    "    medianCPI = medianCPI.set_index('date')\n",
    "    medianCPI = medianCPI[~medianCPI.index.duplicated(keep='first')]\n",
    "\n",
    "    # Read in rGDP\n",
    "    rGDP = pd.read_csv(get_load_data_large_dir() / 'macro' / 'realGDP.csv')\n",
    "    rGDP.columns = ['date', 'rGDP']\n",
    "    rGDP['date'] = pd.to_datetime(rGDP['date'])\n",
    "    rGDP['date'] = (rGDP['date'] + pd.DateOffset(months=3))\n",
    "    rGDP = rGDP.loc[rGDP.index.repeat(3)]\n",
    "    rGDP = rGDP.set_index('date')\n",
    "    rGDP = rGDP[~rGDP.index.duplicated(keep='first')]\n",
    "\n",
    "    # Read in rIR\n",
    "    rIR = pd.read_csv(get_load_data_large_dir() / 'macro' / 'realInterestRate.csv')\n",
    "    rIR.columns = ['date', 'rIR']\n",
    "    rIR['date'] = pd.to_datetime(rIR['date'])\n",
    "    rIR['date'] = (rIR['date'] + pd.DateOffset(months=1))\n",
    "    rIR = rIR.set_index('date')\n",
    "    rIR = rIR[~rIR.index.duplicated(keep='first')]\n",
    "\n",
    "    # Read in UR\n",
    "    UR = pd.read_csv(get_load_data_large_dir() / 'macro' / 'unemploymentRate.csv')\n",
    "    UR.columns = ['date', 'UR']\n",
    "    UR['date'] = pd.to_datetime(UR['date'])\n",
    "    UR['date'] = (UR['date'] + pd.DateOffset(months=1))\n",
    "    UR = UR.set_index('date')\n",
    "    UR = UR[~UR.index.duplicated(keep='first')]\n",
    "\n",
    "    # Read in TB\n",
    "    TB = pd.read_csv(get_load_data_large_dir() / 'macro' / 'TB.csv')\n",
    "    TB.columns = ['date', 'TB']\n",
    "    TB['date'] = pd.to_datetime(TB['date'])\n",
    "    TB['date'] = (TB['date'] + pd.DateOffset(months=1))\n",
    "    TB = TB.set_index('date')\n",
    "    TB = TB[~TB.index.duplicated(keep='first')]\n",
    "\n",
    "    # Read in PPI\n",
    "    PPI = pd.read_csv(get_load_data_large_dir() / 'macro' / 'PPI.csv')\n",
    "    PPI.columns = ['date', 'PPI']\n",
    "    PPI['date'] = pd.to_datetime(PPI['date'])\n",
    "    PPI['date'] = (PPI['date'] + pd.DateOffset(months=1))\n",
    "    PPI = PPI.set_index('date')\n",
    "    PPI = PPI[~PPI.index.duplicated(keep='first')]\n",
    "\n",
    "    # Read in Retail Sales\n",
    "    retailSales = pd.read_csv(get_load_data_large_dir() / 'macro' / 'retailSales.csv')\n",
    "    retailSales.columns = ['date', 'retailSales']\n",
    "    retailSales['date'] = pd.to_datetime(retailSales['date'])\n",
    "    retailSales['date'] = (retailSales['date'] + pd.DateOffset(months=1))\n",
    "    retailSales = retailSales.set_index('date')\n",
    "    retailSales = retailSales[~retailSales.index.duplicated(keep='first')]\n",
    "\n",
    "    # Read in Industry Production Index\n",
    "    indProdIndex = pd.read_csv(get_load_data_large_dir() / 'macro' / 'indProdIndex.csv')\n",
    "    indProdIndex.columns = ['date', 'indProdIndex']\n",
    "    indProdIndex['date'] = pd.to_datetime(indProdIndex['date'])\n",
    "    indProdIndex['date'] = (indProdIndex['date'] + pd.DateOffset(months=1))\n",
    "    indProdIndex = indProdIndex.set_index('date')\n",
    "    indProdIndex = indProdIndex[~indProdIndex.index.duplicated(keep='first')]\n",
    "\n",
    "    # Read in Real Disposable Income\n",
    "    realDispoIncome = pd.read_csv(get_load_data_large_dir() / 'macro' / 'realDispoIncome.csv')\n",
    "    realDispoIncome.columns = ['date', 'realDispoIncome']\n",
    "    realDispoIncome['date'] = pd.to_datetime(realDispoIncome['date'])\n",
    "    realDispoIncome['date'] = (realDispoIncome['date'] + pd.DateOffset(months=1))\n",
    "    realDispoIncome = realDispoIncome.set_index('date')\n",
    "    realDispoIncome = realDispoIncome[~realDispoIncome.index.duplicated(keep='first')]\n",
    "\n",
    "    # Calculate percent change\n",
    "    def pctChange(data, name):\n",
    "        data.replace('.', np.nan, inplace=True)\n",
    "        data = data.astype(float)\n",
    "        data[f'{name}_pct']=data[f'{name}'].pct_change()\n",
    "        return data\n",
    "\n",
    "    # Execute function\n",
    "    IF = pctChange(IF, '5YIF')\n",
    "    medianCPI = pctChange(medianCPI, 'medCPI')\n",
    "    rGDP = pctChange(rGDP, 'rGDP')\n",
    "    rIR = pctChange(rIR, 'rIR')\n",
    "    UR = pctChange(UR, 'UR')\n",
    "    TB = pctChange(TB, 'TB')\n",
    "    PPI = pctChange(PPI, 'PPI')\n",
    "    retailSales = pctChange(retailSales, 'retailSales')\n",
    "    indProdIndex = pctChange(indProdIndex, 'indProdIndex')\n",
    "    realDispoIncome = pctChange(realDispoIncome, 'realDispoIncome')\n",
    "\n",
    "    # Merge all macro data together\n",
    "    macro = (pd.merge(IF, medianCPI, left_index=True, right_index=True, how='left')\n",
    "                 .merge(rGDP, left_index=True, right_index=True, how='left')\n",
    "                 .merge(rIR, left_index=True, right_index=True, how='left')\n",
    "                 .merge(UR, left_index=True, right_index=True, how='left')\n",
    "                 .merge(TB, left_index=True, right_index=True, how='left')\n",
    "                 .merge(PPI, left_index=True, right_index=True, how='left')\n",
    "                 .merge(retailSales, left_index=True, right_index=True, how='left')\n",
    "                 .merge(indProdIndex, left_index=True, right_index=True, how='left')\n",
    "                 .merge(realDispoIncome, left_index=True, right_index=True, how='left'))\n",
    "    factor_macro = macro[['5YIF_pct', 'medCPI_pct', 'rGDP_pct', 'rIR_pct', 'UR_pct', 'TB_pct', 'PPI_pct', 'retailSales_pct', 'indProdIndex_pct', 'realDispoIncome_pct']]\n",
    "    factor_macro = factor_macro.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # Forward Fill by max 31 days\n",
    "    factor_macro = ffill_max_days(factor_macro, 31)\n",
    "\n",
    "    # Set timeframe\n",
    "    factor_macro = factor_macro.loc['2005-01-01': '2023-01-01']\n",
    "\n",
    "    # Export data\n",
    "    factor_macro.to_parquet(get_load_data_parquet_dir() / 'data_macro.parquet.brotli', compression='brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bb3697d576d3e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T19:34:45.762488900Z",
     "start_time": "2023-08-15T19:34:35.114159700Z"
    }
   },
   "outputs": [],
   "source": [
    "create_macro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7588e22543ff09c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T19:34:45.816487600Z",
     "start_time": "2023-08-15T19:34:45.766488200Z"
    }
   },
   "outputs": [],
   "source": [
    "macro = pd.read_parquet(get_load_data_parquet_dir() / 'data_macro.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d4e021-beef-43bb-8a78-a396cc8a0b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "macro.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c584461ae8388b7",
   "metadata": {},
   "source": [
    "# SPY Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b3dc9d50b04643",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T19:13:41.355094600Z",
     "start_time": "2023-08-15T19:13:41.326094800Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_spy_return():\n",
    "    # Get SPY returns\n",
    "    spy_return = get_spy('2005-01-01', '2023-01-01')\n",
    "    spy_return.index.name = 'date'\n",
    "    spy_return.to_parquet(get_load_data_parquet_dir() / 'data_spy.parquet.brotli', compression = 'brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aead082bf9c15ac2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T19:13:43.122333800Z",
     "start_time": "2023-08-15T19:13:42.460472900Z"
    }
   },
   "outputs": [],
   "source": [
    "create_spy_return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1cfa6485265f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T19:13:49.136059500Z",
     "start_time": "2023-08-15T19:13:49.104073600Z"
    }
   },
   "outputs": [],
   "source": [
    "spy_return = pd.read_parquet(get_load_data_parquet_dir() / 'data_spy.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f02435-e304-449a-90f7-c4b66d8cd12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_return.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96315b9a",
   "metadata": {},
   "source": [
    "# Fund Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32d8d954-23cf-4e42-9658-a61ceb6994e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fund_raw():\n",
    "    # Read in dataset\n",
    "    fund_raw = pd.read_csv(get_load_data_large_dir() / 'crsp_compustat_fund.csv')\n",
    "\n",
    "    # Rename columns\n",
    "    fund_raw.columns = fund_raw.columns.str.lower()\n",
    "\n",
    "    # Keep only the most recent data for each fiscal quarter\n",
    "    fund_raw = fund_raw.sort_values(by=['gvkey', 'fyearq', 'fqtr', 'datadate'])\n",
    "    fund_raw = fund_raw.groupby(['gvkey', 'fyearq', 'fqtr']).last().reset_index()\n",
    "    \n",
    "    # Convert to datetime\n",
    "    fund_raw['datadate'] = pd.to_datetime(fund_raw['datadate'])\n",
    "    fund_raw['rdq'] = pd.to_datetime(fund_raw['rdq'])\n",
    "\n",
    "    # Shift data 3 months forward\n",
    "    fund_raw['time_avail_m'] = (fund_raw['datadate'] + pd.DateOffset(months=3)).dt.to_period('M')\n",
    "    fund_raw.loc[(~fund_raw['rdq'].isnull()) & (fund_raw['rdq'].dt.to_period('M') > fund_raw['time_avail_m']), 'time_avail_m'] = fund_raw['rdq'].dt.to_period('M')\n",
    "    \n",
    "    # Compute month difference\n",
    "    month_diff = (fund_raw['rdq'] - fund_raw['datadate']).dt.days // 30\n",
    "    fund_raw = fund_raw.drop(fund_raw[(month_diff > 6) & ~fund_raw['rdq'].isnull()].index)\n",
    "    fund_raw = fund_raw.sort_values(by=['gvkey', 'time_avail_m', 'datadate'])\n",
    "\n",
    "    # Keep most recent data\n",
    "    fund_raw = fund_raw.groupby(['gvkey', 'time_avail_m']).last().reset_index()\n",
    "\n",
    "    # Create extra yearly columns\n",
    "    for col in ['sstky', 'prstkcy', 'oancfy', 'fopty']:\n",
    "        grouped = fund_raw.groupby(['gvkey', 'fyearq'])[col]\n",
    "        condition = fund_raw['fqtr'] == 1\n",
    "        new_values = np.where(condition, fund_raw[col], fund_raw[col] - grouped.shift(1))\n",
    "        fund_raw[col + 'q'] = new_values\n",
    "        \n",
    "    # Convert index from quarterly to monthly\n",
    "    fund_raw = fund_raw.loc[fund_raw.index.repeat(3)]\n",
    "    fund_raw['tempTimeAvailM'] = fund_raw['time_avail_m']\n",
    "    fund_raw = fund_raw.sort_values(by=['gvkey', 'tempTimeAvailM'])\n",
    "    fund_raw['time_avail_m'] = fund_raw.groupby(['gvkey', 'tempTimeAvailM']).cumcount() + fund_raw['time_avail_m']\n",
    "    \n",
    "    # Sort values\n",
    "    fund_raw = fund_raw.sort_values(by=['gvkey', 'time_avail_m', 'datadate'])\n",
    "    # Keep most recent data\n",
    "    fund_raw = fund_raw.groupby(['gvkey', 'time_avail_m']).last().reset_index()\n",
    "    fund_raw = fund_raw.drop(columns=['tempTimeAvailM'])\n",
    "    fund_raw = fund_raw.rename(columns={'datadate': 'datadateq', 'time_avail_m':'date', 'lpermno':'permno'})\n",
    "\n",
    "    # Convert from YY-MM to YY-MM-DD (2012-01 to 2012-01-31)\n",
    "    fund_raw.date = fund_raw.date.dt.to_timestamp(\"M\")\n",
    "    fund_raw = fund_raw.set_index(['permno', 'date'])\n",
    "\n",
    "    # Export list of stocks\n",
    "    stock = read_stock(get_load_data_large_dir() / 'permno_to_train_comp.csv')\n",
    "\n",
    "    # Convert data to numerical format (exclude columns that are not numerical format)\n",
    "    fund_raw = get_stocks_data(fund_raw, stock)\n",
    "    numeric_cols = fund_raw.select_dtypes(include=['number']).columns\n",
    "    fund_raw[numeric_cols] = fund_raw[numeric_cols].astype(float)\n",
    "    non_numeric_cols = fund_raw.select_dtypes(exclude=['number']).columns\n",
    "    fund_raw_numeric = fund_raw[numeric_cols]\n",
    "\n",
    "    # Export list of stocks (these will be used for training)\n",
    "    export_stock(fund_raw, get_load_data_large_dir() / 'permno_to_train_fund.csv')\n",
    "    fund_raw_numeric = fund_raw_numeric.sort_index(level=['permno', 'date'])\n",
    "\n",
    "    # Forward fill yearly data\n",
    "    cols_to_fill = [col for col in fund_raw_numeric.columns if col.endswith('y')]\n",
    "    fund_raw_numeric[cols_to_fill] = fund_raw_numeric[cols_to_fill].ffill()\n",
    "\n",
    "    # Export data\n",
    "    fund_raw_numeric.to_parquet(get_load_data_parquet_dir() / 'data_fund_raw.parquet.brotli', compression='brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d00ce29-bd4d-4ea8-b57b-50f64a39d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_fund_raw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7bdd58-117d-4ac0-9192-9ad516749eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_raw = pd.read_parquet(get_load_data_parquet_dir() / 'data_fund_raw.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e65c93-4e3a-4080-9f88-4f0868d64428",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_fund = read_stock(get_load_data_large_dir() / 'permno_to_train_fund.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6630d987-a4a1-4cfa-939f-f00805e2029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stock_fund)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60647fdc-7c29-4907-b7e9-c085318b36f6",
   "metadata": {},
   "source": [
    "# Fund Raw Annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b61fa5d-022e-4667-acb8-afee96fdbd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fund_raw_a():\n",
    "    # Read in annual\n",
    "    annual = pd.read_csv(get_load_data_large_dir() / 'crsp_compustat_fund_a.csv')\n",
    "\n",
    "    # Rename columns \n",
    "    annual.columns = annual.columns.str.lower()\n",
    "    annual = annual.rename(columns={'lpermno':'permno'})\n",
    "    \n",
    "    # Drop rows based on condition\n",
    "    annual = annual.dropna(subset=['at', 'prcc_c', 'ni'])\n",
    "    \n",
    "    # Extract 6 digits from CUSIP\n",
    "    annual['cnum'] = annual['cusip'].str[:6]\n",
    "    \n",
    "    # Replacing missing values\n",
    "    annual['dr'] = annual.apply(lambda row: row['drc'] + row['drlt'] if pd.notna(row['drc']) and pd.notna(row['drlt']) else (row['drc'] if pd.notna(row['drc']) else (row['drlt'] if pd.notna(row['drlt']) else None)), axis=1)\n",
    "    annual.loc[(annual['dcpstk'] > annual['pstk']) & pd.notna(annual['dcpstk']) & pd.notna(annual['pstk']) & pd.isna(annual['dcvt']), 'dc'] = annual['dcpstk'] - annual['pstk']\n",
    "    annual.loc[pd.isna(annual['pstk']) & pd.notna(annual['dcpstk']) & pd.isna(annual['dcvt']), 'dc'] = annual['dcpstk']\n",
    "    annual.loc[pd.isna(annual['dc']), 'dc'] = annual['dcvt']\n",
    "    annual['xint0'] = annual['xint'].fillna(0)\n",
    "    annual['xsga0'] = annual['xsga'].fillna(0)\n",
    "    annual['xad0'] = annual.apply(lambda row: 0 if row['xad'] < 0 else row['xad'], axis=1)\n",
    "    vars_list = ['nopi', 'dvt', 'ob', 'dm', 'dc', 'aco', 'ap', 'intan', 'ao', 'lco', 'lo', 'rect', 'invt', 'drc', 'spi', 'gdwl', 'che', 'dp', 'act', 'lct', 'tstkp', 'dvpa', 'scstkc', 'sstk', 'mib', 'ivao', 'prstkc', 'prstkcc', 'txditc', 'ivst']\n",
    "    for var in vars_list:\n",
    "        annual[var].fillna(0, inplace=True)\n",
    "\n",
    "    # Shift data forward by 6 months\n",
    "    annual['date'] = pd.to_datetime(annual['datadate']).dt.to_period('M') + 6\n",
    "\n",
    "    # Convert index from annually to monthly\n",
    "    annual = annual.reindex(annual.index.repeat(12))\n",
    "    annual['tempTime'] = annual.groupby(['gvkey', 'date']).cumcount()\n",
    "    annual['date'] += annual['tempTime']\n",
    "    annual = annual.drop(columns=['tempTime'])\n",
    "    \n",
    "    # Convert from YY-MM to YY-MM-DD (2012-01 to 2012-01-31)\n",
    "    annual.date = annual.date.dt.to_timestamp(\"M\")\n",
    "    annual = annual.drop('datadate', axis=1)\n",
    "\n",
    "    # Set index and remove duplicate indices\n",
    "    annual = annual.set_index(['permno', 'date'])\n",
    "    annual = annual.sort_index(level=['permno', 'date'])\n",
    "    annual = annual[~annual.index.duplicated(keep='first')]\n",
    "\n",
    "    # Read in list of stock and apply get_stocks_data\n",
    "    stock = read_stock(get_load_data_large_dir() / 'permno_to_train_fund.csv')\n",
    "    annual = get_stocks_data(annual, stock)\n",
    "\n",
    "    # Export data\n",
    "    annual.to_parquet(get_load_data_parquet_dir() / 'data_fund_raw_a.parquet.brotli', compression='brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1979b53d-4e7b-4302-994e-e86817650fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_fund_raw_a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67816cd-3f1f-4994-882a-710be661f918",
   "metadata": {},
   "outputs": [],
   "source": [
    "annual = pd.read_parquet(get_load_data_parquet_dir() / 'data_fund_raw_a.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdc33ca-f171-477b-ab18-8cb1c779fde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(get_stock_idx(annual))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93de887-6255-4869-b5e6-3b8d38c6eb6f",
   "metadata": {},
   "source": [
    "# Pension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48f354b4-9e95-457d-950c-92f1dc53221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pension():\n",
    "    # Read in pension\n",
    "    pension = pd.read_csv(get_load_data_large_dir() / 'pension_compustat.csv')\n",
    "\n",
    "    # Rename columns\n",
    "    pension.columns = pension.columns.str.lower()\n",
    "\n",
    "    # Drop duplicate indices\n",
    "    pension = pension.sort_values(by=['gvkey', 'datadate'])\n",
    "    pension = pension.groupby(['gvkey', 'datadate']).last().reset_index()\n",
    "    \n",
    "    # Convert to datetime and set index\n",
    "    pension['datadate'] = pd.to_datetime(pension['datadate'])\n",
    "    pension = pension.rename(columns = {'datadate': 'date', 'tic': 'ticker'})\n",
    "    pension = pension.set_index('date')\n",
    "\n",
    "    # Shift everything 1 year forward\n",
    "    for col in pension.columns:\n",
    "        if col != 'gvkey' or col != 'indfmt' or col != 'datafmt' or col != 'consol' or col != 'popsrc' or col != 'ticker':\n",
    "            pension[col] = pension.groupby('gvkey')[col].shift(1)\n",
    "\n",
    "    # Export data\n",
    "    pension.to_parquet(get_load_data_parquet_dir() / 'data_pension.parquet.brotli', compression='brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68530e68-1938-4fb6-9727-013693b378ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e278efba-381b-42e1-92fc-ca2c6490323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pension = pd.read_parquet(get_load_data_parquet_dir() / 'data_pension.parquet.brotli')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74bbc33",
   "metadata": {},
   "source": [
    "# Open Asset Pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e2785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_open_asset_pricing():\n",
    "    # Read in open asset\n",
    "    oap_data = pd.read_parquet(get_load_data_large_dir() / 'open_asset.parquet.brotli')\n",
    "\n",
    "    # Convert to datetime and set index\n",
    "    oap_data['date'] = pd.to_datetime(oap_data['yyyymm'], format='%Y%m')\n",
    "    oap_data = oap_data.drop(['yyyymm'], axis=1)\n",
    "    oap_data = oap_data.set_index(['permno', 'date'])\n",
    "    oap_data = oap_data.sort_index(level=['permno', 'date'])\n",
    "\n",
    "    # Find overlapping permnos between open asset dataset and my dataset\n",
    "    stocks = read_stock(get_load_data_large_dir() / 'permno_to_train_fund.csv')\n",
    "    oap_data = get_stocks_data(oap_data, stocks)\n",
    "    oap_data.to_parquet(get_load_data_parquet_dir() / 'data_open_asset.parquet.brotli', compression='brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001c2caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_open_asset_pricing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59795b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_asset = pd.read_parquet(get_load_data_parquet_dir() / 'data_open_asset.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4291f30d-65d8-4376-9abd-4b58a8e15fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "oap_data = pd.read_parquet(get_load_data_large_dir() / 'open_asset.parquet.brotli')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algofacto",
   "language": "python",
   "name": "algofacto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
