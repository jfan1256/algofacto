{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6c7bbc",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48fcd7a0",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas_datareader.data as web\n",
    "import polars as pl\n",
    "\n",
    "from functions.utils.func import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3c3c98",
   "metadata": {},
   "source": [
    "# CRSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ded06e95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T19:25:03.652284900Z",
     "start_time": "2023-08-01T19:25:03.636039800Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_crsp():\n",
    "    crsp = pd.read_parquet(get_load_data_large_dir() / 'price.parquet.brotli')\n",
    "    crsp['date'] = pd.to_datetime(crsp['date'])\n",
    "    print('Finished pd.to_datetime')\n",
    "    \n",
    "    # Remove stocks that aren't over $100 Million Market Cap (Average market cap over stock timeline)\n",
    "    cap = crsp[['PERMNO', 'date', 'PRC', 'SHROUT']]\n",
    "    cap = cap.rename(columns={'PERMNO':'permno', 'PRC':'Close' , 'SHROUT':'out_share'})\n",
    "    cap = cap.set_index(['permno', 'date']).sort_index(level=['permno', 'date'])\n",
    "    cap = cap[~cap.index.duplicated(keep='first')]\n",
    "    cap = cap.astype(float)\n",
    "    cap['market_cap'] = cap['Close'] * cap['out_share'] * 1000\n",
    "    avg_cap = cap.groupby('permno')['market_cap'].mean()\n",
    "    above_mill = avg_cap[avg_cap > 10_000_000_000].index\n",
    "    cap = cap[cap.index.get_level_values('permno').isin(above_mill)]\n",
    "    print('Finished market capitalization')\n",
    "    \n",
    "    # Remove stocks that do not have at least 2 years worth of year data\n",
    "    cap = set_length(cap, 2)\n",
    "    \n",
    "    # Remove stocks that have more than 1 NAN values in their Closing price column\n",
    "    # Stocks that get delisted have 1 row of NAN values as their last row\n",
    "    # Stocks that switch ticker (WM to COOP: 81593) have rows of NAN valuescap = cap.dropna(subset='Close')\n",
    "    # Afterwards, drop all rows that have NAN values in Close (every delisted permno stock only has 1 NAN in Close now)\n",
    "    nan_counts = cap.groupby('permno')['Close'].apply(lambda x: x.isna().sum())\n",
    "    valid_permnos = nan_counts[nan_counts <= 1].index.tolist()\n",
    "    cap = cap[cap.index.get_level_values('permno').isin(valid_permnos)]\n",
    "    cap = cap.dropna(subset='Close')\n",
    "    cap = cap.drop(columns=['Close', 'out_share'], axis=1)\n",
    "    \n",
    "    export_stock(cap, get_load_data_large_dir() / 'permno_to_train.csv')\n",
    "    stock = read_stock(get_load_data_large_dir() / 'permno_to_train.csv')\n",
    "    print('Finished exporting stock list')\n",
    "    \n",
    "    # There are some duplicates in WRD Indices (they are all the same value)\n",
    "    data = crsp[['PERMNO', 'date', 'OPENPRC', 'ASKHI', 'BIDLO', 'PRC', 'VOL', 'TICKER', 'SHROUT']]\n",
    "    data = data.rename(columns={'PERMNO':'permno', 'OPENPRC':'Open', 'ASKHI':'High', \n",
    "                                'BIDLO':'Low', 'PRC':'Close', 'VOL':'Volume', 'TICKER':'ticker', \n",
    "                                'SHROUT':'out_share'})\n",
    "    \n",
    "    data = data.set_index(['permno', 'date']).sort_index(level=['permno', 'date'])\n",
    "    data = get_stocks_data(data, stock)\n",
    "    data = data[~data.index.duplicated(keep='first')]\n",
    "    data = data.dropna(subset='Close')\n",
    "    \n",
    "    # Price\n",
    "    ohclv = data[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "    ohclv = ohclv.astype(float)\n",
    "    ohclv = get_stocks_data(ohclv, stock)\n",
    "    print('Finished Price')\n",
    "    \n",
    "    # Ticker\n",
    "    ticker = data[['ticker']]\n",
    "    print('Finished Ticker')\n",
    "\n",
    "    # Outstanding Share\n",
    "    out = data[['out_share']]\n",
    "    out['out_share'] = out['out_share'] * 1000\n",
    "    print('Finished Outstanding Share')\n",
    "    \n",
    "    # Date\n",
    "    date = data.drop(columns=data.columns)\n",
    "    print('Finished Date')\n",
    "    \n",
    "    print('Exporting...')\n",
    "    ohclv.to_parquet(get_load_data_parquet_dir() / 'data_price.parquet.brotli')\n",
    "    ticker.to_parquet(get_load_data_parquet_dir() / 'data_ticker.parquet.brotli')\n",
    "    out.to_parquet(get_load_data_parquet_dir() / 'data_out.parquet.brotli')\n",
    "    date.to_parquet(get_load_data_parquet_dir() / 'data_date.parquet.brotli')\n",
    "    cap.to_parquet(get_load_data_parquet_dir() / 'data_cap.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "12d7ffe6c9457314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished pd.to_datetime\n",
      "Finished market capitalization\n",
      "Finished exporting stock list\n",
      "Finished Price\n",
      "Finished Ticker\n",
      "Finished Outstanding Share\n",
      "Finished Date\n",
      "Exporting...\n"
     ]
    }
   ],
   "source": [
    "create_crsp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1df0e660",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohclv = pd.read_parquet(get_load_data_parquet_dir() / 'data_price.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "757548b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = pd.read_parquet(get_load_data_parquet_dir() / 'data_ticker.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d6ef8eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.read_parquet(get_load_data_parquet_dir() / 'data_out.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "531f19b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = pd.read_parquet(get_load_data_parquet_dir() / 'data_date.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "56ad84b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = pd.read_parquet(get_load_data_parquet_dir() / 'data_cap.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9a95e3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = read_stock(get_load_data_large_dir() / 'permno_to_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30daaf6",
   "metadata": {},
   "source": [
    "# Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b72f9b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ind():\n",
    "    # Assign industry based off range\n",
    "    def assign_label(df, column_name, sic_ranges, label):\n",
    "        df['sic_temp'] = df['sic']\n",
    "\n",
    "        for r in sic_ranges:\n",
    "            if isinstance(r, tuple):\n",
    "                df.loc[(df['sic_temp'] >= r[0]) & (df['sic_temp'] <= r[1]), f'{column_name}'] = label\n",
    "            else:\n",
    "                df.loc[df['sic_temp'] == r, f'{column_name}'] = label\n",
    "\n",
    "        df = df.drop(columns=['sic_temp'])\n",
    "        return df\n",
    "\n",
    "    price_data = pd.read_parquet(get_load_data_large_dir() / 'price.parquet.brotli')\n",
    "    price_data['date'] = pd.to_datetime(price_data['date'])\n",
    "    print('Finished pd.to_datetime')\n",
    "\n",
    "    ind = price_data[['PERMNO', 'date', 'SICCD', 'PRC']]\n",
    "    ind = ind.rename(columns={'PERMNO':'permno', 'SICCD':'sic'})\n",
    "    ind = ind.set_index(['permno', 'date']).sort_index(level=['permno', 'date'])\n",
    "    ind = ind[~ind.index.duplicated(keep='first')]\n",
    "    ind = get_stocks_data(ind, stock)\n",
    "    ind = ind.dropna(subset='PRC')\n",
    "    ind = ind.drop('PRC', axis=1)\n",
    "    \n",
    "\n",
    "    fama_ind = {\n",
    "    'agric' : [(100, 199), (200, 299), (700, 799), (910, 919), 2048],\n",
    "    'food' : [(2000, 2009), (2010, 2019), (2020, 2029), (2030, 2039), (2040, 2046), (2050, 2059), (2060, 2063), (2070, 2079),(2090, 2092), 2095, (2098, 2099)],\n",
    "    'soda' : [(2064, 2068), 2086, 2087, 2096, 2097],\n",
    "    'beer' : [2080, 2082, 2083, 2084, 2085],\n",
    "    'smoke' : [(2100, 2199)],\n",
    "    'toys' : [(920, 999), (3650, 3651), 3652, 3732, (3930, 3931), (3940, 3949)],\n",
    "    'fun' : [(7800, 7829), (7830, 7833), (7840, 7841), 7900, (7910, 7911),(7920, 7929), (7930, 7933), (7940, 7949), 7980, (7990, 7999)],\n",
    "    'books' : [(2700, 2709), (2710, 2719), (2720, 2729), (2730, 2739), (2740, 2749), (2770, 2771), (2780, 2789), (2790, 2799)],\n",
    "    'hshld' : [2047, (2391, 2392), (2510, 2519), (2590, 2599), (2840, 2843), 2844, (3160, 3161), (3170, 3171), 3172, (3190, 3199), 3229, 3260, (3262, 3263), 3269, (3230, 3231), (3630, 3639), (3750, 3751), 3800, (3860, 3861), (3870, 3873), (3910, 3911), 3914, 3915, (3960, 3962), 3991, 3995],\n",
    "    'clths' : [(2300, 2390), (3020, 3021), (3100, 3111), (3130, 3131), (3140, 3149), (3150, 3151), (3963, 3965)],\n",
    "    'hlth' : [(8000, 8099)],\n",
    "    'medeq' : [3693, (3840, 3849), (3850, 3851)],\n",
    "    'drugs' : [2830, 2831, 2833, 2834, 2835, 2836],\n",
    "    'chems' : [(2800, 2809), (2810, 2819), (2820, 2829), (2850, 2859), (2860, 2869), (2870, 2879), (2890, 2899)],\n",
    "    'rubbr' : [3031, 3041, (3050, 3053), (3060, 3069), (3070, 3079), (3080, 3089), (3090, 3099)],\n",
    "    'txtls' : [(2200, 2269), (2270, 2279), (2280, 2284), (2290, 2295), 2297, 2298, 2299, (2393, 2395), (2397, 2399)],\n",
    "    'bldmt' : [(800, 899), (2400, 2439), (2450, 2459), (2490, 2499), (2660, 2661), (2950, 2952), 3200, (3210, 3211), (3240, 3241), (3250, 3259), 3261, 3264, (3270, 3275), (3280, 3281), (3290, 3293), (3295, 3299), (3420, 3429), (3430, 3433), (3440, 3441), 3442, 3446, 3448, 3449, (3450, 3451), 3452, (3490, 3499), 3996],\n",
    "    'cnstr' : [(1500, 1511), (1520, 1529), (1530, 1539), (1540, 1549), (1600, 1699), (1700, 1799)],\n",
    "    'steel' : [(3300, 3300), (3310, 3317), (3320, 3325), (3330, 3339), (3340, 3341), (3350, 3357), (3360, 3369), (3370, 3379), (3390, 3399)],\n",
    "    'fabpr' : [(3400, 3400), (3443, 3443), (3444, 3444), (3460, 3469), (3470, 3479)],\n",
    "    'mach' : [(3510, 3519), (3520, 3529), (3530, 3530), (3531, 3531), (3532, 3532), (3533, 3533), (3534, 3534), (3535, 3535), (3536, 3536), (3538, 3538), (3540, 3549), (3550, 3559), (3560, 3569), (3580, 3580), (3581, 3581), (3582, 3582), (3585, 3585), (3586, 3586), (3589, 3589), (3590, 3599)],\n",
    "    'elceq' : [(3600, 3600), (3610, 3613), (3620, 3621), (3623, 3629), (3640, 3644), (3645, 3645), (3646, 3646), (3648, 3649), (3660, 3660), (3690, 3690), (3691, 3692), (3699, 3699)],\n",
    "    'autos' : [(2296, 2296), (2396, 2396), (3010, 3011), (3537, 3537), (3647, 3647), (3694, 3694), (3700, 3700), (3710, 3710), (3711, 3711), (3713, 3713), (3714, 3714), (3715, 3715), (3716, 3716), (3792, 3792), (3790, 3791), (3799, 3799)],\n",
    "    'aero' : [(3720, 3720), (3721, 3721), (3723, 3724), (3725, 3725), (3728, 3729)],\n",
    "    'ships' : [(3730, 3731), (3740, 3743)],\n",
    "    'guns' : [(3760, 3769), (3795, 3795), (3480, 3489)],\n",
    "    'gold' : [(1040, 1049)],\n",
    "    'mines' : [(1000, 1009), (1010, 1019), (1020, 1029), (1030, 1039), (1050, 1059), (1060, 1069), (1070, 1079), (1080, 1089), (1090, 1099), (1100, 1119), (1400, 1499)],\n",
    "    'coal' : [(1200, 1299)],\n",
    "    'oil' : [(1300, 1300), (1310, 1319), (1320, 1329), (1330, 1339), (1370, 1379), (1380, 1380), (1381, 1381), (1382, 1382), (1389, 1389), (2900, 2912), (2990, 2999)],\n",
    "    'util' : [(4900, 4900), (4910, 4911), (4920, 4922), (4923, 4923), (4924, 4925), (4930, 4931), (4932, 4932), (4939, 4939), (4940, 4942)],\n",
    "    'telcm' : [(4800, 4800), (4810, 4813), (4820, 4822), (4830, 4839), (4840, 4841), (4880, 4889), (4890, 4890), (4891, 4891), (4892, 4892), (4899, 4899)],\n",
    "    'persv' : [(7020, 7021), (7030, 7033), (7200, 7200), (7210, 7212), (7214, 7214), (7215, 7216), (7217, 7217), (7219, 7219), (7220, 7221), (7230, 7231), (7240, 7241), (7250, 7251), (7260, 7269), (7270, 7290), (7291, 7291), (7292, 7299), (7395, 7395), (7500, 7500), (7520, 7529), (7530, 7539), (7540, 7549), (7600, 7600), (7620, 7620), (7622, 7622), (7623, 7623), (7629, 7629), (7630, 7631), (7640, 7641), (7690, 7699), (8100, 8199), (8200, 8299), (8300, 8399), (8400, 8499), (8600, 8699), (8800, 8899), (7510, 7515)],\n",
    "    'bussv' : [(2750, 2759), (3993, 3993), (7218, 7218), (7300, 7300), (7310, 7319), (7320, 7329), (7330, 7339),(7340, 7342), (7349, 7349), (7350, 7351), (7352, 7352), (7353, 7353), (7359, 7359), (7360, 7369),(7370, 7372), (7374, 7374), (7375, 7375), (7376, 7376), (7377, 7377), (7378, 7378), (7379, 7379),(7380, 7380), (7381, 7382), (7383, 7383), (7384, 7384), (7385, 7385), (7389, 7390), (7391, 7391),(7392, 7392), (7393, 7393), (7394, 7394), (7396, 7396), (7397, 7397), (7399, 7399), (7519, 7519),(8700, 8700), (8710, 8713), (8720, 8721), (8730, 8734), (8740, 8748), (8900, 8910), (8911, 8911),(8920, 8999), (4220, 4229)],\n",
    "    'comps' : [(3570, 3579), (3680, 3680), (3681, 3681), (3682, 3682), (3683, 3683), (3684, 3684), (3685, 3685),(3686, 3686), (3687, 3687), (3688, 3688), (3689, 3689), (3695, 3695), (7373, 7373)],\n",
    "    'chips' : [(3622, 3622), (3661, 3661), (3662, 3662), (3663, 3663), (3664, 3664), (3665, 3665), (3666, 3666),(3669, 3669), (3670, 3679), (3810, 3810), (3812, 3812)],\n",
    "    'labeq' : [(3811, 3811), (3820, 3820), (3821, 3821), (3822, 3822), (3823, 3823), (3824, 3824), (3825, 3825),(3826, 3826), (3827, 3827), (3829, 3829), (3830, 3839)],\n",
    "    'paper' : [(2520, 2549), (2600, 2639), (2670, 2699), (2760, 2761), (3950, 3955)],\n",
    "    'boxes' : [(2440, 2449), (2640, 2659), (3220, 3221), (3410, 3412)],\n",
    "    'whlsl' : [(5000, 5000), (5010, 5015), (5020, 5023), (5030, 5039), (5040, 5042), (5043, 5043), (5044, 5044), (5045, 5045), (5046, 5046), (5047, 5047), (5048, 5048), (5049, 5049), (5050, 5059), (5060, 5060), (5063, 5063), (5064, 5064), (5065, 5065), (5070, 5078), (5080, 5080), (5081, 5081), (5082, 5082), (5083, 5083), (5084, 5084), (5085, 5085), (5086, 5087), (5088, 5088), (5090, 5090), (5091, 5092), (5093, 5093), (5094, 5094), (5099, 5099), (5100, 5100), (5110, 5113), (5120, 5122), (5130, 5139), (5140, 5149), (5150, 5159), (5160, 5169), (5170, 5172), (5180, 5182), (5190, 5199)],\n",
    "    'trans' : [(4000, 4013), (4040, 4049), (4100, 4100), (4110, 4119), (4120, 4121), (4130, 4131), (4140, 4142),(4150, 4151), (4170, 4173), (4190, 4199), (4200, 4200), (4210, 4219), (4230, 4231), (4240, 4249),(4400, 4499), (4500, 4599), (4600, 4699), (4700, 4700), (4710, 4712), (4720, 4729), (4730, 4739),(4740, 4749), (4780, 4780), (4782, 4782), (4783, 4783), (4784, 4784)],\n",
    "    'rtail' : [(5200, 5200), (5210, 5219), (5220, 5229), (5230, 5231), (5250, 5251), (5260, 5261), (5270, 5271), (5300, 5300), (5310, 5311), (5320, 5320), (5330, 5331), (5334, 5334), (5340, 5349), (5390, 5399), (5400, 5400), (5410, 5411), (5412, 5412), (5420, 5429), (5430, 5439), (5440, 5449), (5450, 5459), (5460, 5469), (5490, 5499), (5500, 5500), (5510, 5529), (5530, 5539), (5540, 5549), (5550, 5559), (5560, 5569), (5570, 5579), (5590, 5599), (5600, 5699), (5700, 5700), (5710, 5719), (5720, 5722), (5730, 5733), (5734, 5734), (5735, 5735), (5736, 5736), (5750, 5799), (5900, 5900), (5910, 5912), (5920, 5929), (5930, 5932), (5940, 5940), (5941, 5941), (5942, 5942), (5943, 5943), (5944, 5944), (5945, 5945), (5946, 5946), (5947, 5947), (5948, 5948), (5949, 5949), (5950, 5959), (5960, 5969), (5970, 5979), (5980, 5989), (5990, 5990), (5992, 5992), (5993, 5993), (5994, 5994), (5995, 5995), (5999, 5999)],\n",
    "    'meals' : [(5800, 5819), (5820, 5829), (5890, 5899), (7000, 7000), (7010, 7019), (7040, 7049), (7213, 7213)],\n",
    "    'banks' : [(6000, 6000), (6010, 6019), (6020, 6020), (6021, 6021), (6022, 6022), (6023, 6024), (6025, 6025), (6026, 6026), (6027, 6027), (6028, 6029), (6030, 6036), (6040, 6059), (6060, 6062), (6080, 6082), (6090, 6099), (6100, 6100), (6110, 6111), (6112, 6113), (6120, 6129), (6130, 6139), (6140, 6149), (6150, 6159), (6160, 6169), (6170, 6179), (6190, 6199)],\n",
    "    'insur' : [(6300, 6300), (6310, 6319), (6320, 6329), (6330, 6331), (6350, 6351), (6360, 6361), (6370, 6379), (6390, 6399), (6400, 6411)],\n",
    "    'rlest' : [(6500, 6500), (6510, 6510), (6512, 6512), (6513, 6513), (6514, 6514), (6515, 6515), (6517, 6519), (6520, 6529), (6530, 6531), (6532, 6532), (6540, 6541), (6550, 6553), (6590, 6599), (6610, 6611)],\n",
    "    'fin' : [(6200, 6299), (6700, 6700), (6710, 6719), (6720, 6722), (6723, 6723), (6724, 6724), (6725, 6725), (6726, 6726), (6730, 6733), (6740, 6779), (6790, 6791), (6792, 6792), (6793, 6793), (6794, 6794), (6795, 6795), (6798, 6798), (6799, 6799)]\n",
    "    # 'other' : [(4950, 4959), (4960, 4961), (4970, 4971), (4990, 4991)]\n",
    "    }\n",
    "\n",
    "    for name, ranges in fama_ind.items():\n",
    "        print('-'*60)\n",
    "        print(name)\n",
    "        ind = assign_label(ind, 'ind', ranges, name)\n",
    "\n",
    "    ind['ind'], category_mapping = ind['ind'].factorize()\n",
    "\n",
    "    ind = ind.drop('sic', axis=1)\n",
    "    ind.to_parquet(get_load_data_parquet_dir() / 'data_ind.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c6cfdeed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished pd.to_datetime\n",
      "------------------------------------------------------------\n",
      "agric\n",
      "------------------------------------------------------------\n",
      "food\n",
      "------------------------------------------------------------\n",
      "soda\n",
      "------------------------------------------------------------\n",
      "beer\n",
      "------------------------------------------------------------\n",
      "smoke\n",
      "------------------------------------------------------------\n",
      "toys\n",
      "------------------------------------------------------------\n",
      "fun\n",
      "------------------------------------------------------------\n",
      "books\n",
      "------------------------------------------------------------\n",
      "hshld\n",
      "------------------------------------------------------------\n",
      "clths\n",
      "------------------------------------------------------------\n",
      "hlth\n",
      "------------------------------------------------------------\n",
      "medeq\n",
      "------------------------------------------------------------\n",
      "drugs\n",
      "------------------------------------------------------------\n",
      "chems\n",
      "------------------------------------------------------------\n",
      "rubbr\n",
      "------------------------------------------------------------\n",
      "txtls\n",
      "------------------------------------------------------------\n",
      "bldmt\n",
      "------------------------------------------------------------\n",
      "cnstr\n",
      "------------------------------------------------------------\n",
      "steel\n",
      "------------------------------------------------------------\n",
      "fabpr\n",
      "------------------------------------------------------------\n",
      "mach\n",
      "------------------------------------------------------------\n",
      "elceq\n",
      "------------------------------------------------------------\n",
      "autos\n",
      "------------------------------------------------------------\n",
      "aero\n",
      "------------------------------------------------------------\n",
      "ships\n",
      "------------------------------------------------------------\n",
      "guns\n",
      "------------------------------------------------------------\n",
      "gold\n",
      "------------------------------------------------------------\n",
      "mines\n",
      "------------------------------------------------------------\n",
      "coal\n",
      "------------------------------------------------------------\n",
      "oil\n",
      "------------------------------------------------------------\n",
      "util\n",
      "------------------------------------------------------------\n",
      "telcm\n",
      "------------------------------------------------------------\n",
      "persv\n",
      "------------------------------------------------------------\n",
      "bussv\n",
      "------------------------------------------------------------\n",
      "comps\n",
      "------------------------------------------------------------\n",
      "chips\n",
      "------------------------------------------------------------\n",
      "labeq\n",
      "------------------------------------------------------------\n",
      "paper\n",
      "------------------------------------------------------------\n",
      "boxes\n",
      "------------------------------------------------------------\n",
      "whlsl\n",
      "------------------------------------------------------------\n",
      "trans\n",
      "------------------------------------------------------------\n",
      "rtail\n",
      "------------------------------------------------------------\n",
      "meals\n",
      "------------------------------------------------------------\n",
      "banks\n",
      "------------------------------------------------------------\n",
      "insur\n",
      "------------------------------------------------------------\n",
      "rlest\n",
      "------------------------------------------------------------\n",
      "fin\n"
     ]
    }
   ],
   "source": [
    "create_ind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8045ecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = pd.read_parquet(get_load_data_parquet_dir() / 'data_ind.parquet.brotli')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e590d2b",
   "metadata": {},
   "source": [
    "# Fama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "45a6b51e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-03T19:21:56.862578200Z",
     "start_time": "2023-08-03T19:21:56.843341Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_fama():\n",
    "    fama_data = (web.DataReader('F-F_Research_Data_5_Factors_2x3_daily', 'famafrench', start=2005)[0].rename(columns={'Mkt-RF': 'MARKET'}))\n",
    "    fama_data.index.names = ['date']\n",
    "    fama_data = fama_data.astype(float)\n",
    "    fama_data.to_parquet(get_load_data_parquet_dir() / 'data_fama.parquet.brotli', compression='brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5a3da433256bf0c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-03T19:21:59.133594400Z",
     "start_time": "2023-08-03T19:21:57.469391700Z"
    }
   },
   "outputs": [],
   "source": [
    "create_fama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d54bf078bed94a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "fama = pd.read_parquet(get_load_data_parquet_dir() / 'data_fama.parquet.brotli')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734bc03d",
   "metadata": {},
   "source": [
    "# ETF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "783e39cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-03T19:21:42.010959700Z",
     "start_time": "2023-08-03T19:21:41.948971Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_etf():\n",
    "    etf_tickers = read_stock(get_load_data_large_dir() / 'tickers_etf.csv')[1:]\n",
    "    start_date = '2005-01-01'\n",
    "    end_date = '2023-01-01'\n",
    "    etf_data = yf.download(etf_tickers, start=start_date, end=end_date)\n",
    "    etf_data = etf_data.stack().swaplevel().sort_index()\n",
    "    etf_data.index.names = ['ticker', 'date']\n",
    "    etf_data = etf_data.astype(float)\n",
    "\n",
    "    # Calculate returns of each ticker and rename each return column to ticker\n",
    "    ret = etf_data.groupby('ticker')['Close'].apply(lambda x: x.pct_change())\n",
    "    ret_df = ret.unstack(level='ticker')\n",
    "    dates = etf_data.reset_index('ticker').drop(\n",
    "        ['ticker', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'], axis=1)\n",
    "    dates = dates.loc[~dates.index.duplicated(keep='first')].sort_index()\n",
    "    etf_data = pd.concat([dates, ret_df], axis=1)\n",
    "    etf_data.to_parquet(get_load_data_parquet_dir() / 'data_etf.parquet.brotli', compression='brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b8e358571eff1182",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-03T19:21:45.497396500Z",
     "start_time": "2023-08-03T19:21:43.209472400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  10 of 10 completed\n"
     ]
    }
   ],
   "source": [
    "create_etf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "21b0977f334fe3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "etf = pd.read_parquet(get_load_data_parquet_dir() / 'data_etf.parquet.brotli')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb11edc",
   "metadata": {},
   "source": [
    "# Macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "41579ea4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T19:34:34.389161700Z",
     "start_time": "2023-08-15T19:34:34.285161400Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_macro():\n",
    "    IF = pd.read_csv(get_load_data_large_dir() / 'macro' / 'fiveYearIR.csv')\n",
    "    IF.columns = ['date', '5YIF']\n",
    "    IF = IF.set_index(pd.to_datetime(IF['date'])).drop('date', axis=1)\n",
    "\n",
    "    medianCPI = pd.read_csv(get_load_data_large_dir() / 'macro' / 'medianCPI.csv')\n",
    "    medianCPI.columns = ['date', 'medCPI']\n",
    "    medianCPI = medianCPI.set_index(pd.to_datetime(medianCPI['date'])).drop('date', axis=1)\n",
    "    medianCPI = medianCPI.shift(1)\n",
    "\n",
    "    rGDP = pd.read_csv(get_load_data_large_dir() / 'macro' / 'realGDP.csv')\n",
    "    rGDP.columns = ['date', 'rGDP']\n",
    "    rGDP = rGDP.set_index(pd.to_datetime(rGDP['date'])).drop('date', axis=1)\n",
    "\n",
    "    rIR = pd.read_csv(get_load_data_large_dir() / 'macro' / 'realInterestRate.csv')\n",
    "    rIR.columns = ['date', 'rIR']\n",
    "    rIR = rIR.set_index(pd.to_datetime(rIR['date'])).drop('date', axis=1)\n",
    "    rIR = rIR.shift(1)\n",
    "\n",
    "    UR = pd.read_csv(get_load_data_large_dir() / 'macro' / 'unemploymentRate.csv')\n",
    "    UR.columns = ['date', 'UR']\n",
    "    UR = UR.set_index(pd.to_datetime(UR['date'])).drop('date', axis=1)\n",
    "    UR = UR.shift(1)\n",
    "\n",
    "    TB = pd.read_csv(get_load_data_large_dir() / 'macro' / 'TB.csv')\n",
    "    TB.columns = ['date', 'TB']\n",
    "    TB = TB.set_index(pd.to_datetime(TB['date'])).drop('date', axis=1)\n",
    "    TB = TB.shift(1)\n",
    "    \n",
    "    PPI = pd.read_csv(get_load_data_large_dir() / 'macro' / 'PPI.csv')\n",
    "    PPI.columns = ['date', 'PPI']\n",
    "    PPI = PPI.set_index(pd.to_datetime(PPI['date'])).drop('date', axis=1)\n",
    "    PPI = PPI.shift(1)\n",
    "    \n",
    "    retailSales = pd.read_csv(get_load_data_large_dir() / 'macro' / 'retailSales.csv')\n",
    "    retailSales.columns = ['date', 'retailSales']\n",
    "    retailSales = retailSales.set_index(pd.to_datetime(retailSales['date'])).drop('date', axis=1)\n",
    "    retailSales = retailSales.shift(1)\n",
    "    \n",
    "    indProdIndex = pd.read_csv(get_load_data_large_dir() / 'macro' / 'indProdIndex.csv')\n",
    "    indProdIndex.columns = ['date', 'indProdIndex']\n",
    "    indProdIndex = indProdIndex.set_index(pd.to_datetime(indProdIndex['date'])).drop('date', axis=1)\n",
    "    indProdIndex = indProdIndex.shift(1)\n",
    "\n",
    "    realDispoIncome = pd.read_csv(get_load_data_large_dir() / 'macro' / 'realDispoIncome.csv')\n",
    "    realDispoIncome.columns = ['date', 'realDispoIncome']\n",
    "    realDispoIncome = realDispoIncome.set_index(pd.to_datetime(realDispoIncome['date'])).drop('date', axis=1)\n",
    "    realDispoIncome = realDispoIncome.shift(1)\n",
    "    \n",
    "    def pctChange(data, name):\n",
    "        data.replace('.', np.nan, inplace=True)\n",
    "        data = data.astype(float)\n",
    "        data[f'{name}_pct']=data[f'{name}'].pct_change()\n",
    "        return data\n",
    "    \n",
    "    IF = pctChange(IF, '5YIF')\n",
    "    medianCPI = pctChange(medianCPI, 'medCPI')\n",
    "    rGDP = pctChange(rGDP, 'rGDP')\n",
    "    rIR = pctChange(rIR, 'rIR')\n",
    "    UR = pctChange(UR, 'UR')\n",
    "    TB = pctChange(TB, 'TB')\n",
    "    PPI = pctChange(PPI, 'PPI')\n",
    "    retailSales = pctChange(retailSales, 'retailSales')\n",
    "    indProdIndex = pctChange(indProdIndex, 'indProdIndex')\n",
    "    realDispoIncome = pctChange(realDispoIncome, 'realDispoIncome')\n",
    "    \n",
    "    macro = (pd.merge(IF, medianCPI, left_index=True, right_index=True, how='left').ffill()\n",
    "                 .merge(rGDP, left_index=True, right_index=True, how='left').ffill()\n",
    "                 .merge(rIR, left_index=True, right_index=True, how='left').ffill()\n",
    "                 .merge(UR, left_index=True, right_index=True, how='left').ffill()\n",
    "                 .merge(TB, left_index=True, right_index=True, how='left').ffill()\n",
    "                 .merge(PPI, left_index=True, right_index=True, how='left').ffill()\n",
    "                 .merge(retailSales, left_index=True, right_index=True, how='left').ffill()\n",
    "                 .merge(indProdIndex, left_index=True, right_index=True, how='left').ffill()\n",
    "                 .merge(realDispoIncome, left_index=True, right_index=True, how='left').ffill())\n",
    "    \n",
    "    factor_macro = macro[['5YIF_pct', 'medCPI_pct', 'rGDP_pct', 'rIR_pct', 'UR_pct', 'TB_pct', 'PPI_pct', 'retailSales_pct', 'indProdIndex_pct', 'realDispoIncome_pct']]\n",
    "    \n",
    "#     def normalize(df):\n",
    "#         df = (df[-1]-df.mean())/df.std()\n",
    "#         return df\n",
    "    \n",
    "#     factor_macro['5YIF_pct'] = factor_macro['5YIF_pct'].rolling(30).apply(lambda x: normalize(x))\n",
    "#     factor_macro['medCPI_pct'] = factor_macro['medCPI_pct'].rolling(30).apply(lambda x: normalize(x))\n",
    "#     factor_macro['rGDP_pct'] = factor_macro['rGDP_pct'].rolling(30).apply(lambda x: normalize(x))\n",
    "#     factor_macro['rIR_pct'] = factor_macro['rIR_pct'].rolling(30).apply(lambda x: normalize(x))\n",
    "#     factor_macro['UR_pct'] = factor_macro['UR_pct'].rolling(30).apply(lambda x: normalize(x))\n",
    "#     factor_macro['TB_pct'] = factor_macro['TB_pct'].rolling(30).apply(lambda x: normalize(x))\n",
    "#     factor_macro['PPI_pct'] = factor_macro['PPI_pct'].rolling(30).apply(lambda x: normalize(x))\n",
    "#     factor_macro['retailSales_pct'] = factor_macro['retailSales_pct'].rolling(30).apply(lambda x: normalize(x))\n",
    "#     factor_macro['indProdIndex_pct'] = factor_macro['indProdIndex_pct'].rolling(30).apply(lambda x: normalize(x))\n",
    "#     factor_macro['realDispoIncome_pct'] = factor_macro['realDispoIncome_pct'].rolling(30).apply(lambda x: normalize(x))\n",
    "\n",
    "    \n",
    "#     factor_macro['medCPI_div_rGDP'] = (macro['medCPI'] / macro['rGDP']).pct_change()\n",
    "#     factor_macro['5YIF_div_medCPI'] = (macro['5YIF']/macro['medCPI']).pct_change()\n",
    "    \n",
    "    factor_macro = factor_macro.replace([np.inf, -np.inf], np.nan)\n",
    "    factor_macro.to_parquet(get_load_data_parquet_dir() / 'data_macro.parquet.brotli', compression='brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "79bb3697d576d3e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T19:34:45.762488900Z",
     "start_time": "2023-08-15T19:34:35.114159700Z"
    }
   },
   "outputs": [],
   "source": [
    "create_macro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7588e22543ff09c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T19:34:45.816487600Z",
     "start_time": "2023-08-15T19:34:45.766488200Z"
    }
   },
   "outputs": [],
   "source": [
    "macro = pd.read_parquet(get_load_data_parquet_dir() / 'data_macro.parquet.brotli')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60196f96",
   "metadata": {},
   "source": [
    "# PCA Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "181e8fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pca_return():\n",
    "    # Read in price data and set time frame and remove data with less than 2 years length of data (same data as create_factor.py)\n",
    "    price_data = pd.read_parquet(get_load_data_parquet_dir() / 'data_price.parquet.brotli')\n",
    "    price_data = set_timeframe(price_data, '2005-01-01', '2023-01-01')\n",
    "    price_data = set_length(price_data, year=2)\n",
    "\n",
    "    # Create returns and convert ticker index to columns\n",
    "    price_data = create_return(price_data, windows=[1])\n",
    "    ret = price_data[[f'RET_01']]\n",
    "    ret = ret['RET_01'].unstack(price_data.index.names[0])\n",
    "    ret.iloc[0] = ret.iloc[0].fillna(0)\n",
    "\n",
    "    # Execute Rolling PCA\n",
    "    window_size=60\n",
    "    num_components=5\n",
    "    pca_return = rolling_pca(data=ret, window_size=window_size, num_components=num_components, name='Return')\n",
    "    pca_return.to_parquet(get_load_data_parquet_dir() / 'data_pca_ret.parquet.brotli', compression='brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "173eb908e12e1d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pca_return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "118d16d4d1216ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_ret = pd.read_parquet(get_load_data_parquet_dir() / 'data_pca_ret.parquet.brotli')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687cf656663499a8",
   "metadata": {},
   "source": [
    "# All RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9f22ecbda1ea627f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T04:59:51.021950200Z",
     "start_time": "2023-08-18T04:59:51.006222Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_all_rf():\n",
    "    etf_data = pd.read_parquet(get_load_data_parquet_dir() / 'data_etf.parquet.brotli')\n",
    "    fama_data = pd.read_parquet(get_load_data_parquet_dir() / 'data_fama.parquet.brotli')\n",
    "    pca_data = pd.read_parquet(get_load_data_parquet_dir() / 'data_pca_ret.parquet.brotli')\n",
    "    macro_data = pd.read_parquet(get_load_data_parquet_dir() / 'data_macro.parquet.brotli')\n",
    "    all_rf = pd.concat([etf_data, fama_data, pca_data, macro_data], axis=1)\n",
    "    all_rf = set_timeframe(all_rf, '2005-01-01', '2023-01-01')\n",
    "    fama_data = set_timeframe(fama_data, '2005-01-01', '2023-01-01')\n",
    "    # Execute Rolling PCA\n",
    "    window_size=60\n",
    "    num_components=5\n",
    "    pca_rf = rolling_pca(data=all_rf, window_size=window_size, num_components=num_components, name='RF')\n",
    "    # Add risk-free rate\n",
    "    pca_rf = pd.concat([pca_rf, fama_data['RF']], axis=1)    \n",
    "    pca_rf.to_parquet(get_load_data_parquet_dir() / 'data_all_rf.parquet.brotli', compression = 'brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "94c22790e32c72d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T04:59:51.280300800Z",
     "start_time": "2023-08-18T04:59:51.021950200Z"
    }
   },
   "outputs": [],
   "source": [
    "create_all_rf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9ea83c9000b47430",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T04:59:51.493998300Z",
     "start_time": "2023-08-18T04:59:51.452168100Z"
    }
   },
   "outputs": [],
   "source": [
    "all_rf = pd.read_parquet(get_load_data_parquet_dir() / 'data_all_rf.parquet.brotli')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c584461ae8388b7",
   "metadata": {},
   "source": [
    "# SPY Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c5b3dc9d50b04643",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T19:13:41.355094600Z",
     "start_time": "2023-08-15T19:13:41.326094800Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_spy_return():\n",
    "    spy_return = get_spy('2005-01-01', '2023-01-01')\n",
    "    spy_return.index.name = 'date'\n",
    "    spy_return.to_parquet(get_load_data_parquet_dir() / 'data_spy.parquet.brotli', compression = 'brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "aead082bf9c15ac2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T19:13:43.122333800Z",
     "start_time": "2023-08-15T19:13:42.460472900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "create_spy_return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5e1cfa6485265f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T19:13:49.136059500Z",
     "start_time": "2023-08-15T19:13:49.104073600Z"
    }
   },
   "outputs": [],
   "source": [
    "spy_return = pd.read_parquet(get_load_data_parquet_dir() / 'data_spy.parquet.brotli')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74bbc33",
   "metadata": {},
   "source": [
    "# Open Asset Pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "680e2785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_open_asset_pricing():\n",
    "    oap_data = pd.read_parquet(get_load_data_large_dir() / 'signed_predictors_dl_wide.parquet.brotli')\n",
    "    permno_codes = pd.read_csv(get_load_data_large_dir() / 'permno.csv')\n",
    "    factors_to_use = ['DivSeason', 'ChTax', 'EarningsStreak', 'ResidualMomentum', 'AssetGrowth',\n",
    "                  'NOA', 'SmileSlope', 'MomSeasonShort', 'InvestPPEInv', 'NetDebtFinance', 'InvGrowth', 'MomSeason11YrPlus']\n",
    "\n",
    "    oap_data = oap_data[['permno', 'yyyymm'] + factors_to_use]\n",
    "    permno_codes = permno_codes[['LPERMNO', 'tic']].rename(columns={'LPERMNO':'permno'})\n",
    "\n",
    "    permno_unique = permno_codes.drop_duplicates().sort_values(by='permno')\n",
    "\n",
    "    permno_unique = dict(zip(permno_unique['permno'], permno_unique['tic']))\n",
    "\n",
    "    oap_filtered = oap_data[oap_data['permno'].isin(permno_unique.keys())]\n",
    "    oap_filtered['tic'] = oap_filtered['permno'].map(permno_unique)\n",
    "\n",
    "    oap_filtered['date'] = pd.to_datetime(oap_filtered['yyyymm'], format='%Y%m')\n",
    "    oap_filtered.rename(columns={'tic':'ticker'}, inplace=True)\n",
    "    oap_filtered.drop(['permno', 'yyyymm'], axis=1, inplace=True)\n",
    "    oap_filtered.set_index(['ticker', 'date'], inplace=True)\n",
    "    oap_filtered.sort_index(level=['ticker', 'date'], inplace=True)\n",
    "    \n",
    "    # Find overlapping tickers\n",
    "    current_tickers = read_ticker(get_load_data_large_dir() / 'tickers_to_train_fundamental.csv')\n",
    "    oap_tickers = get_ticker_idx(oap_filtered)\n",
    "    overlapping_tickers = list(set(oap_tickers) & set(current_tickers))\n",
    "\n",
    "    # Filter DataFrame based on overlapping tickers\n",
    "    oap_filtered = oap_filtered[oap_filtered.index.get_level_values('ticker').isin(overlapping_tickers)]\n",
    "    \n",
    "    export_ticker(oap_filtered, get_load_data_large_dir() / 'tickers_to_train_open.csv')\n",
    "    oap_filtered.to_parquet(get_load_data_parquet_dir() / 'data_open_asset.parquet.brotli', compression='brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "001c2caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_open_asset_pricing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59795b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_asset = pd.read_parquet(get_load_data_parquet_dir() / 'data_open_asset.parquet.brotli')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d15c03",
   "metadata": {},
   "source": [
    "# Dividend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "19cec509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dividend():\n",
    "    dividend = pd.read_csv(get_load_data_large_dir() / 'dividend.csv')\n",
    "    dividend = dividend.drop(['PERMNO'], axis=1)\n",
    "    dividend.columns = ['date', 'ticker', 'divdecdt', 'divpaydt', 'divrecdt', 'distcode', 'divpay']\n",
    "    dividend['date'] = pd.to_datetime(dividend['date'])\n",
    "    dividend['divdecdt'] = pd.to_datetime(dividend['divdecdt'])\n",
    "    dividend['divpaydt'] = pd.to_datetime(dividend['divpaydt'])\n",
    "    dividend['divrecdt'] = pd.to_datetime(dividend['divrecdt'])\n",
    "    dividend = dividend.set_index(['ticker', 'date']).sort_index(level=['ticker', 'date'])\n",
    "    dividend = dividend[dividend.index.get_level_values('ticker').notna()]\n",
    "\n",
    "    mask = ~dividend['distcode'].astype(str).str.startswith('12')\n",
    "    dividend[mask] = np.nan\n",
    "    dividend.to_parquet(get_load_data_parquet_dir() / 'data_dividend.parquet.brotli', compression='brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "db2016ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dividend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "02745ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dividend = pd.read_parquet(get_load_data_parquet_dir() / 'data_dividend.parquet.brotli')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4b13d1",
   "metadata": {},
   "source": [
    "# Fund Ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "19927951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fund_ind():\n",
    "    fund_ind = pd.read_csv(get_load_data_large_dir() / 'fund_ind.csv')\n",
    "\n",
    "    def convert_to_float(cell):\n",
    "        if isinstance(cell, str) and '%' in cell:\n",
    "            return float(cell.replace('%', '')) / 100\n",
    "        return float(cell)\n",
    "\n",
    "    def connect(s):\n",
    "        return '_'.join(word for word in s.split() if word)\n",
    "\n",
    "    fund_ind['public_date'] = pd.to_datetime(fund_ind['public_date'])\n",
    "    fund_ind = fund_ind.rename(columns={'public_date': 'date'}).set_index(['date', 'gicdesc'])\n",
    "    fund_ind = fund_ind.drop(['NFIRM'], axis=1)\n",
    "    fund_ind = fund_ind.fillna(0)\n",
    "\n",
    "    collect = []\n",
    "    for ind, df in fund_ind.groupby('gicdesc'):\n",
    "        ind = connect(ind)\n",
    "        df = df.applymap(convert_to_float)\n",
    "        # Execute Rolling PCA\n",
    "        window_size=5 # 3 months\n",
    "        num_components=5\n",
    "        pca_fund_ind = rolling_pca(data=df, window_size=window_size, num_components=num_components, name=f'FI_{ind}')\n",
    "        pca_fund_ind = pca_fund_ind.reset_index('gicdesc').drop('gicdesc', axis=1)\n",
    "        collect.append(pca_fund_ind)\n",
    "\n",
    "    fund_ind = pd.concat(collect, axis=1)\n",
    "    fund_ind.to_parquet(get_load_data_parquet_dir() / 'data_fund_ind.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8432b668",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_fund_ind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3c9297f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_ind = pd.read_parquet(get_load_data_parquet_dir() / 'data_fund_ind.parquet.brotli')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96315b9a",
   "metadata": {},
   "source": [
    "# Fund Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "7900cdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fund_raw():\n",
    "    fund_raw = pd.read_csv(get_load_data_large_dir() / 'fund_raw.csv')\n",
    "    stock = read_stock(get_load_data_large_dir() / 'permno_to_train.csv')\n",
    "    attributes = ['GVKEY', 'fyearq', 'fyearq', 'indfmt', 'costat', 'consol', 'popsrc', \n",
    "                  'datafmt', 'curcdq', 'datacqtr', 'datafqtr']\n",
    "    fund_raw = fund_raw.drop(attributes, axis=1)\n",
    "    fund_raw['datadate'] = pd.to_datetime(fund_raw['datadate'])\n",
    "    fund_raw = fund_raw.rename(columns={'datadate': 'date', 'LPERMNO': 'permno'}).set_index(['permno', 'date'])\n",
    "    fund_raw = fund_raw.sort_index(level=['permno', 'date'])\n",
    "    fund_raw = fund_raw.drop(columns=fund_raw.columns[fund_raw.isna().sum() > len(fund_raw) / 3])\n",
    "    fund_raw = get_stocks_data(fund_raw, stock)\n",
    "    fund_raw = fund_raw[~fund_raw.index.duplicated(keep='first')]\n",
    "    export_stock(fund_raw, get_load_data_large_dir() / 'permno_to_train_fund.csv')\n",
    "\n",
    "    # Retrieve quarter category\n",
    "    fund_q = fund_raw[['fqtr']]\n",
    "    fund_q = fund_q.fillna(-1)\n",
    "    fund_q = fund_q.astype(int)\n",
    "    fund_raw = fund_raw.drop('fqtr', axis=1)\n",
    "    fund_raw = fund_raw.astype(float)\n",
    "    fund_raw.columns = [col.upper() for col in fund_raw.columns]\n",
    "\n",
    "    before = fund_raw.columns\n",
    "    # Liquidity Ratios\n",
    "    fund_raw['current_ratio'] = (fund_raw['CHEQ'] + fund_raw['AOQ']) / (fund_raw['APQ'] + fund_raw['DLCQ'])\n",
    "    fund_raw['quick_ratio'] = (fund_raw['CHEQ'] + fund_raw['AOQ'] - fund_raw['INVTQ']) / (fund_raw['APQ'] + fund_raw['DLCQ'])\n",
    "\n",
    "    # Leverage Ratios\n",
    "    fund_raw['total_debt'] = fund_raw['DLCQ'] + fund_raw['DLTTQ']\n",
    "    fund_raw['total_equity'] = fund_raw['ATQ'] - fund_raw['LTQ']\n",
    "    fund_raw['debt_to_equity_ratio'] = fund_raw['total_debt'] / fund_raw['total_equity']\n",
    "    fund_raw['debt_ratio'] = fund_raw['total_debt'] / fund_raw['ATQ']\n",
    "\n",
    "    # Profitability Ratios\n",
    "    fund_raw['net_profit_margin'] = fund_raw['NIQ'] / fund_raw['SALEQ']\n",
    "    fund_raw['return_on_assets'] = fund_raw['NIQ'] / fund_raw['ATQ']\n",
    "    fund_raw['return_on_equity'] = fund_raw['NIQ'] / fund_raw['total_equity']\n",
    "    fund_raw['gross_profit_margin'] = (fund_raw['SALEQ'] - fund_raw['COGSQ']) / fund_raw['SALEQ']\n",
    "    fund_raw['operating_margin'] = (fund_raw['REVTQ'] - fund_raw['XSGAQ']) / fund_raw['REVTQ']\n",
    "    fund_raw['return_on_investment'] = (fund_raw['NIQ'] + fund_raw['XINTQ']) / (fund_raw['LTQ'] + fund_raw['total_equity'])\n",
    "\n",
    "    # Efficiency Ratios\n",
    "    fund_raw['inventory_turnover'] = fund_raw['COGSQ'] / fund_raw['INVTQ']\n",
    "    fund_raw['asset_turnover'] = fund_raw['SALEQ'] / fund_raw['ATQ']\n",
    "    fund_raw['receivable_turnover'] = fund_raw['SALEQ'] / fund_raw.get('ARQ', 0.01)  # Assume ARQ is Account Receivable, replace with correct column if different\n",
    "\n",
    "    # Market Ratios\n",
    "    fund_raw['earnings_per_share'] = fund_raw['NIQ'] / fund_raw['CSHOQ']\n",
    "    fund_raw['book_value_per_share'] = fund_raw['total_equity'] / fund_raw['CSHOQ']\n",
    "    # fund_raw['price_to_earnings_ratio'] = fund_raw['stock_price'] / fund_raw['earnings_per_share']  # Uncomment if stock price is available\n",
    "\n",
    "    # Cash Flow Ratios\n",
    "    fund_raw['operating_cash_flow_ratio'] = fund_raw.get('OANCFQ', 1) / fund_raw['LTQ']  # Assume OANCFQ is Operating Activities Net Cash Flow, replace with correct column if different\n",
    "    fund_raw['free_cash_flow'] = fund_raw.get('OANCFQ', 1) - fund_raw['DPQ'] - fund_raw.get('CAPEXQ', 0)  # Assume CAPEXQ is Capital Expenditure, replace with correct column if different\n",
    "\n",
    "    # Handling division by zero and replacing inf with NaN\n",
    "    fund_raw = fund_raw.drop(before, axis=1)\n",
    "    fund_raw.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    # Ranking by each column\n",
    "    fund_rank = fund_raw[['current_ratio']]\n",
    "    for col in fund_raw.columns:\n",
    "        fund_rank[f'{col}_rank'] = fund_raw.groupby('date')[col].rank()\n",
    "\n",
    "    fund_rank = fund_rank.drop(['current_ratio'], axis=1)\n",
    "    \n",
    "    fund_q.to_parquet(get_load_data_parquet_dir() / 'data_fund_q.parquet.brotli')\n",
    "    fund_raw.to_parquet(get_load_data_parquet_dir() / 'data_fund_raw.parquet.brotli')\n",
    "    fund_rank.to_parquet(get_load_data_parquet_dir() / 'data_fund_rank.parquet.brotli', compression='brotli')\n",
    "    \n",
    "#     fund_raw = fund_raw.mul(10**6)\n",
    "#     # Change to daily interval\n",
    "#     date_data = pd.read_parquet(get_load_data_parquet_dir() / 'data_date.parquet.brotli')\n",
    "#     tickers = read_ticker(get_load_data_large_dir() / 'tickers_to_train_fundamental.csv')\n",
    "#     date_data = set_timeframe(date_data, '2004-01-01', '2023-06-01')\n",
    "    \n",
    "#     fund_raw = pd.merge(date_data.loc[tickers], fund_raw, left_index=True, right_index=True, how='left')\n",
    "#     fund_raw = fund_raw.loc[~fund_raw.index.duplicated(keep='first')]\n",
    "#     fund_raw = fund_raw.ffill()\n",
    "    \n",
    "#     fund_q = pd.merge(date_data.loc[tickers], fund_q, left_index=True, right_index=True, how='left')\n",
    "#     fund_q = fund_q.loc[~fund_q.index.duplicated(keep='first')]\n",
    "#     fund_q = fund_q.ffill()\n",
    "#     fund_raw = fund_raw.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "#     # Divide by outstanding shares\n",
    "#     out_data = pd.read_parquet(get_load_data_parquet_dir() / 'data_out.parquet.brotli')\n",
    "#     fund_raw = pd.merge(fund_raw, out_data.out, left_index=True, right_index=True, how='left')\n",
    "#     fund_raw = fund_raw.loc[~fund_raw.index.duplicated(keep='first')]\n",
    "#     fund_raw.iloc[:, :-1] = fund_raw.iloc[:, :-1].div(fund_raw.out, axis=0)\n",
    "#     fund_raw = fund_raw.drop(['out'], axis=1)\n",
    "#     fund_raw = fund_raw.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "#     # Divide by price\n",
    "#     price_data = pd.read_parquet(get_load_data_parquet_dir() / 'data_price.parquet.brotli')\n",
    "#     fund_raw = pd.merge(fund_raw, price_data.Close.loc[tickers], left_index=True, right_index=True, how='left')\n",
    "#     fund_raw = fund_raw.loc[~fund_raw.index.duplicated(keep='first')]\n",
    "#     fund_raw.iloc[:, :-1] = fund_raw.iloc[:, :-1].div(fund_raw.Close, axis=0)\n",
    "#     fund_raw = fund_raw.drop(['Close'], axis=1)\n",
    "#     fund_raw = fund_raw.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "87d525f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_fund_raw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "99672861",
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_q = pd.read_parquet(get_load_data_parquet_dir() / 'data_fund_q.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "91dfd119",
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_raw = pd.read_parquet(get_load_data_parquet_dir() / 'data_fund_raw.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "7ca2a5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_rank = pd.read_parquet(get_load_data_parquet_dir() / 'data_fund_rank.parquet.brotli')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d35d4a6",
   "metadata": {},
   "source": [
    "# Fund Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "963d8ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fund_ratio():\n",
    "    def convert_column_to_numeric(column):\n",
    "        new_column = []\n",
    "        for element in column:\n",
    "            try:\n",
    "                new_column.append(float(element))\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    if element.endswith('%'):\n",
    "                        new_column.append(float(element.rstrip('%')) / 100)\n",
    "                    else:\n",
    "                        new_column.append(np.nan)\n",
    "                except (TypeError, AttributeError):\n",
    "                    new_column.append(np.nan)\n",
    "        return pd.Series(new_column, index=column.index)\n",
    "\n",
    "    fund_ratio = pd.read_csv(get_load_data_large_dir() / 'fund_ratio.csv')\n",
    "    stock = read_stock(get_load_data_large_dir() / 'permno_to_train.csv')\n",
    "    fund_ratio['public_date'] = pd.to_datetime(fund_ratio['public_date'])\n",
    "    fund_ratio = fund_ratio.drop(['adate', 'qdate'], axis=1)\n",
    "    fund_ratio = fund_ratio.rename(columns={'public_date': 'date', 'permno': 'permno'}).set_index(['permno', 'date'])\n",
    "    fund_ratio = fund_ratio.sort_index(level=['permno', 'date'])\n",
    "    fund_ratio = get_stocks_data(fund_ratio, stock)\n",
    "    fund_ratio = fund_ratio.drop(columns=fund_ratio.columns[fund_ratio.isna().sum() > len(fund_ratio) / 3])\n",
    "    for col in fund_ratio.columns:\n",
    "        fund_ratio[col] = convert_column_to_numeric(fund_ratio[col])\n",
    "\n",
    "#     # Multiply monthly data by monthly price\n",
    "#     price_data = pd.read_parquet(get_load_data_parquet_dir() / 'data_price.parquet.brotli')\n",
    "#     complete_date_range = pd.date_range(start=fund_ratio.index.get_level_values('date').min(), end=fund_ratio.index.get_level_values('date').max(), freq='D')\n",
    "#     multi_idx = pd.MultiIndex.from_product([tickers, complete_date_range], names=['ticker', 'date'])\n",
    "#     price_data = price_data.Close.reindex(multi_idx).ffill()\n",
    "#     fund_ratio = pd.merge(fund_ratio, price_data, left_index=True, right_index=True, how='left')\n",
    "#     fund_ratio = fund_ratio.loc[~fund_ratio.index.duplicated(keep='first')]\n",
    "\n",
    "#     fund_ratio.iloc[:, :-1] = fund_ratio.iloc[:, :-1].mul(fund_ratio.Close, axis=0)\n",
    "#     fund_ratio = fund_ratio.drop(['Close'], axis=1)\n",
    "#     fund_ratio = fund_ratio.replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "#     # Divide by price\n",
    "#     price_data = pd.read_parquet(get_load_data_parquet_dir() / 'data_price.parquet.brotli')\n",
    "#     fund_ratio = pd.merge(fund_ratio, price_data.Close.loc[tickers], left_index=True, right_index=True, how='left')\n",
    "#     fund_ratio = fund_ratio.loc[~fund_ratio.index.duplicated(keep='first')]\n",
    "#     fund_ratio.iloc[:, :-1] = fund_ratio.iloc[:, :-1].div(fund_ratio.Close, axis=0)\n",
    "#     fund_ratio = fund_ratio.drop(['Close'], axis=1)\n",
    "#     fund_ratio = fund_ratio.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # Ranking by each column\n",
    "    ratio_rank = fund_ratio[['CAPEI']]\n",
    "    for col in fund_ratio.columns:\n",
    "        ratio_rank[f'{col}_rank'] = fund_ratio.groupby('date')[col].rank(pct=True)\n",
    "\n",
    "    ratio_rank = ratio_rank.drop(['CAPEI'], axis=1)\n",
    "    \n",
    "    fund_ratio.to_parquet(get_load_data_parquet_dir() / 'data_fund_ratio.parquet.brotli', compression='brotli')\n",
    "    ratio_rank.to_parquet(get_load_data_parquet_dir() / 'data_fund_ratio_rank.parquet.brotli', compression='brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "135ffaae94d0b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_fund_ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cf313769b5b792ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T02:35:12.112307100Z",
     "start_time": "2023-08-01T02:35:10.847842500Z"
    }
   },
   "outputs": [],
   "source": [
    "fund_ratio = pd.read_parquet(get_load_data_parquet_dir() / 'data_fund_ratio.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "60171805",
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_ratio_rank = pd.read_parquet(get_load_data_parquet_dir() / 'data_fund_ratio_rank.parquet.brotli')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ffd13a",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e49e7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pca_return_test():\n",
    "    # Read in price data and set time frame and remove data with less than 2 years length of data (same data as create_factor.py)\n",
    "    price_data = pd.read_parquet(get_load_data_parquet_dir() / 'data_price.parquet.brotli')\n",
    "    ticker = read_ticker(get_load_data_large_dir() / 'tickers_to_train_fundamental.csv')\n",
    "    price_data = set_timeframe(price_data, '2002-01-01', '2023-01-01')\n",
    "    price_data = price_data.loc[ticker]\n",
    "\n",
    "    # Create returns and convert ticker index to columns\n",
    "    price_data = create_return(price_data, windows=[1])\n",
    "    ret = price_data[[f'RET_01']]\n",
    "    ret = ret['RET_01'].unstack('ticker')\n",
    "    ret.iloc[0] = ret.iloc[0].fillna(0)\n",
    "\n",
    "    # Execute Rolling PCA\n",
    "    window_size=60\n",
    "    num_components=5\n",
    "    pca_return = rolling_pca(data=ret, window_size=window_size, num_components=num_components, name='Return')\n",
    "    pca_return.to_parquet(get_load_data_parquet_dir() / 'data_pca_ret_test.parquet.brotli', compression='brotli')\n",
    "\n",
    "def create_all_rf_test():\n",
    "    etf_data = pd.read_parquet(get_load_data_parquet_dir() / 'data_etf.parquet.brotli')\n",
    "    fama_data = pd.read_parquet(get_load_data_parquet_dir() / 'data_fama.parquet.brotli')\n",
    "    pca_data = pd.read_parquet(get_load_data_parquet_dir() / 'data_pca_ret_test.parquet.brotli')\n",
    "    macro_data = pd.read_parquet(get_load_data_parquet_dir() / 'data_macro.parquet.brotli')\n",
    "    all_rf = pd.concat([etf_data, fama_data, pca_data, macro_data], axis=1)\n",
    "    all_rf = set_timeframe(all_rf, '2002-01-01', '2023-01-01')\n",
    "    fama_data = set_timeframe(fama_data, '2002-01-01', '2023-01-01')\n",
    "    # Execute Rolling PCA\n",
    "    window_size=60\n",
    "    num_components=5\n",
    "    pca_rf = rolling_pca(data=all_rf, window_size=window_size, num_components=num_components, name='RF')\n",
    "    # Add risk-free rate\n",
    "    pca_rf = pd.concat([pca_rf, fama_data['RF']], axis=1)    \n",
    "    pca_rf.to_parquet(get_load_data_parquet_dir() / 'data_all_rf_test.parquet.brotli', compression = 'brotli')\n",
    "    \n",
    "def create_macro_test():\n",
    "    IF = pd.read_csv(get_load_data_large_dir() / 'macro' / 'fiveYearIR.csv')\n",
    "    IF.columns = ['date', '5YIF']\n",
    "    IF = IF.set_index(pd.to_datetime(IF['date'])).drop('date', axis=1)\n",
    "\n",
    "    medianCPI = pd.read_csv(get_load_data_large_dir() / 'macro' / 'medianCPI.csv')\n",
    "    medianCPI.columns = ['date', 'medCPI']\n",
    "    medianCPI = medianCPI.set_index(pd.to_datetime(medianCPI['date'])).drop('date', axis=1)\n",
    "    medianCPI = medianCPI.shift(1)\n",
    "\n",
    "    rGDP = pd.read_csv(get_load_data_large_dir() / 'macro' / 'realGDP.csv')\n",
    "    rGDP.columns = ['date', 'rGDP']\n",
    "    rGDP = rGDP.set_index(pd.to_datetime(rGDP['date'])).drop('date', axis=1)\n",
    "\n",
    "    rIR = pd.read_csv(get_load_data_large_dir() / 'macro' / 'realInterestRate.csv')\n",
    "    rIR.columns = ['date', 'rIR']\n",
    "    rIR = rIR.set_index(pd.to_datetime(rIR['date'])).drop('date', axis=1)\n",
    "    rIR = rIR.shift(1)\n",
    "\n",
    "    UR = pd.read_csv(get_load_data_large_dir() / 'macro' / 'unemploymentRate.csv')\n",
    "    UR.columns = ['date', 'UR']\n",
    "    UR = UR.set_index(pd.to_datetime(UR['date'])).drop('date', axis=1)\n",
    "    UR = UR.shift(1)\n",
    "\n",
    "    TB = pd.read_csv(get_load_data_large_dir() / 'macro' / 'TB.csv')\n",
    "    TB.columns = ['date', 'TB']\n",
    "    TB = TB.set_index(pd.to_datetime(TB['date'])).drop('date', axis=1)\n",
    "    TB = TB.shift(1)\n",
    "    \n",
    "    PPI = pd.read_csv(get_load_data_large_dir() / 'macro' / 'PPI.csv')\n",
    "    PPI.columns = ['date', 'PPI']\n",
    "    PPI = PPI.set_index(pd.to_datetime(PPI['date'])).drop('date', axis=1)\n",
    "    PPI = PPI.shift(1)\n",
    "    \n",
    "    retailSales = pd.read_csv(get_load_data_large_dir() / 'macro' / 'retailSales.csv')\n",
    "    retailSales.columns = ['date', 'retailSales']\n",
    "    retailSales = retailSales.set_index(pd.to_datetime(retailSales['date'])).drop('date', axis=1)\n",
    "    retailSales = retailSales.shift(1)\n",
    "    \n",
    "    indProdIndex = pd.read_csv(get_load_data_large_dir() / 'macro' / 'indProdIndex.csv')\n",
    "    indProdIndex.columns = ['date', 'indProdIndex']\n",
    "    indProdIndex = indProdIndex.set_index(pd.to_datetime(indProdIndex['date'])).drop('date', axis=1)\n",
    "    indProdIndex = indProdIndex.shift(1)\n",
    "\n",
    "    realDispoIncome = pd.read_csv(get_load_data_large_dir() / 'macro' / 'realDispoIncome.csv')\n",
    "    realDispoIncome.columns = ['date', 'realDispoIncome']\n",
    "    realDispoIncome = realDispoIncome.set_index(pd.to_datetime(realDispoIncome['date'])).drop('date', axis=1)\n",
    "    realDispoIncome = realDispoIncome.shift(1)\n",
    "    \n",
    "    def pctChange(data, name):\n",
    "        data.replace('.', np.nan, inplace=True)\n",
    "        data = data.astype(float)\n",
    "        data[f'{name}_pct']=data[f'{name}'].pct_change()\n",
    "        return data\n",
    "    \n",
    "    IF = pctChange(IF, '5YIF')\n",
    "    medianCPI = pctChange(medianCPI, 'medCPI')\n",
    "    rGDP = pctChange(rGDP, 'rGDP')\n",
    "    rIR = pctChange(rIR, 'rIR')\n",
    "    UR = pctChange(UR, 'UR')\n",
    "    TB = pctChange(TB, 'TB')\n",
    "    PPI = pctChange(PPI, 'PPI')\n",
    "    retailSales = pctChange(retailSales, 'retailSales')\n",
    "    indProdIndex = pctChange(indProdIndex, 'indProdIndex')\n",
    "    realDispoIncome = pctChange(realDispoIncome, 'realDispoIncome')\n",
    "    \n",
    "    macro = (pd.merge(IF, medianCPI, left_index=True, right_index=True, how='left').ffill()\n",
    "                 .merge(rGDP, left_index=True, right_index=True, how='left').ffill()\n",
    "                 .merge(rIR, left_index=True, right_index=True, how='left').ffill()\n",
    "                 .merge(UR, left_index=True, right_index=True, how='left').ffill()\n",
    "                 .merge(TB, left_index=True, right_index=True, how='left').ffill()\n",
    "                 .merge(PPI, left_index=True, right_index=True, how='left').ffill()\n",
    "                 .merge(retailSales, left_index=True, right_index=True, how='left').ffill()\n",
    "                 .merge(indProdIndex, left_index=True, right_index=True, how='left').ffill()\n",
    "                 .merge(realDispoIncome, left_index=True, right_index=True, how='left').ffill())\n",
    "    \n",
    "    factor_macro = macro[['5YIF_pct', 'medCPI_pct', 'rGDP_pct', 'rIR_pct', 'UR_pct', 'TB_pct', 'PPI_pct', 'retailSales_pct', 'indProdIndex_pct', 'realDispoIncome_pct']]\n",
    "    \n",
    "    def normalize(df):\n",
    "        df = (df[-1]-df.mean())/df.std()\n",
    "        return df\n",
    "    \n",
    "    factor_macro['5YIF_pct'] = factor_macro['5YIF_pct'].rolling(30).apply(lambda x: normalize(x))\n",
    "    factor_macro['medCPI_pct'] = factor_macro['medCPI_pct'].rolling(30).apply(lambda x: normalize(x))\n",
    "    factor_macro['rGDP_pct'] = factor_macro['rGDP_pct'].rolling(30).apply(lambda x: normalize(x))\n",
    "    factor_macro['rIR_pct'] = factor_macro['rIR_pct'].rolling(30).apply(lambda x: normalize(x))\n",
    "    factor_macro['UR_pct'] = factor_macro['UR_pct'].rolling(30).apply(lambda x: normalize(x))\n",
    "    factor_macro['TB_pct'] = factor_macro['TB_pct'].rolling(30).apply(lambda x: normalize(x))\n",
    "    factor_macro['PPI_pct'] = factor_macro['PPI_pct'].rolling(30).apply(lambda x: normalize(x))\n",
    "    factor_macro['retailSales_pct'] = factor_macro['retailSales_pct'].rolling(30).apply(lambda x: normalize(x))\n",
    "    factor_macro['indProdIndex_pct'] = factor_macro['indProdIndex_pct'].rolling(30).apply(lambda x: normalize(x))\n",
    "    factor_macro['realDispoIncome_pct'] = factor_macro['realDispoIncome_pct'].rolling(30).apply(lambda x: normalize(x))\n",
    "    \n",
    "    factor_macro = factor_macro.replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "    factor_macro.to_parquet(get_load_data_parquet_dir() / 'data_macro_test.parquet.brotli', compression='brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7b59b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pca_return_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6662135",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_all_rf_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "568e8c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_macro_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantPycharm",
   "language": "python",
   "name": "quantpycharm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
