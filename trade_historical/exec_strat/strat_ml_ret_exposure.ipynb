{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd91e6bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T20:10:46.417178900Z",
     "start_time": "2023-08-05T20:10:44.180386900Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yfinance as yf\n",
    "import quantstats as qs\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import cvxopt as opt\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import base64\n",
    "import cvxpy as cp\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from io import BytesIO\n",
    "from datetime import timedelta\n",
    "from IPython.display import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from sklearn import cluster, covariance, manifold\n",
    "from cvxopt import blas, solvers\n",
    "from IPython.display import display\n",
    "from scipy.optimize import minimize\n",
    "from matplotlib.backends.backend_svg import FigureCanvasSVG\n",
    "\n",
    "from class_model.model_prep import ModelPrep\n",
    "from core.system import *\n",
    "from core.operation import *\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b935ba89-1729-47a1-b0b0-5972ac479c3d",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c776d11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-04T02:43:29.326250800Z",
     "start_time": "2023-08-04T02:43:29.291716700Z"
    }
   },
   "outputs": [],
   "source": [
    "#Reads in all the results in the \"modelName\" folder\n",
    "def read_result(model_name, result_name):\n",
    "    data = []\n",
    "    result_data_dir = get_ml_result_model(live, model_name)\n",
    "    for i, folder_name in enumerate(os.listdir(result_data_dir)):\n",
    "        try:\n",
    "            if folder_name.startswith(\"params\"):\n",
    "                folder_path = os.path.join(result_data_dir, folder_name)\n",
    "                file_path = os.path.join(folder_path, f\"{result_name}.parquet.brotli\")\n",
    "                print(os.path.basename(folder_path))\n",
    "                data.append(pd.read_parquet(file_path))\n",
    "        except:\n",
    "            continue\n",
    "    return pd.concat(data, axis=0).reset_index(drop=True)\n",
    "\n",
    "#Calculates the product of the daily_ic_mean and maximum overall IC in each result to find the best performing model\n",
    "def get_max_ic(data):\n",
    "    collection={}\n",
    "    for index, row in data.iterrows():\n",
    "        collection[max(row.loc[(row.index.str.startswith(\"daily_metric\"))])]=index\n",
    "    max_ic_idx=collection[max(list(collection.keys()))]\n",
    "    return data.iloc[max_ic_idx]\n",
    "\n",
    "#Gets the files of the best performing model\n",
    "def get_max_ic_file(data, model_name):\n",
    "    files = {}\n",
    "    time_index = data.to_frame().index.get_loc('time')\n",
    "    param_vals = data.iloc[:time_index].values\n",
    "    key = [f'{float(p)}' for p in (param_vals)]\n",
    "    key = '_'.join(key)\n",
    "    \n",
    "    result_data_dir = get_ml_result_model(live, model_name) / f'params_{key}'\n",
    "    for file in os.listdir(result_data_dir):\n",
    "        if file.endswith(\".parquet.brotli\"):\n",
    "            files[extract_first_string(file)]=pd.read_parquet(os.path.join(result_data_dir, file))\n",
    "        elif file.endswith(\".png\"):\n",
    "            img = os.path.join(result_data_dir, file)\n",
    "            files[extract_first_string(file)]=Image(img)\n",
    "    return files\n",
    "\n",
    "def get_all(model_name):\n",
    "    return get_max_ic_file(get_max_ic(read_result(model_name, 'metrics')), model_name)\n",
    "\n",
    "def sign_accuracy(predictions, actual, target_sign, pred):\n",
    "    accuracies = []  # To store accuracies for each ticker\n",
    "    \n",
    "    # Iterate through each ticker and calculate accuracy\n",
    "    for ticker in predictions.index.levels[0]:\n",
    "        ticker_group = predictions.loc[ticker]\n",
    "        actual_group = actual.loc[ticker]\n",
    "        \n",
    "        # Determine if each pair has the same sign\n",
    "        if pred == 'price':\n",
    "            correct_signs = (np.sign(ticker_group) == np.sign(actual_group))\n",
    "        elif pred == 'sign':\n",
    "            ticker_group = np.where(ticker_group > 0.5, 1, -1)\n",
    "            correct_signs = (ticker_group == np.sign(actual_group))\n",
    "            \n",
    "        # Filter by target sign if specified\n",
    "        if target_sign == 'positive':\n",
    "            mask = (np.sign(actual_group) == 1)\n",
    "            correct_signs = correct_signs[mask]\n",
    "        elif target_sign == 'negative':\n",
    "            mask = (np.sign(actual_group) == -1)\n",
    "            correct_signs = correct_signs[mask]\n",
    "        \n",
    "        # Calculate the accuracy and store it\n",
    "        accuracy = np.mean(correct_signs) * 100  # Convert to percentage\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "    # Calculate and return the mean accuracy across all tickers\n",
    "    mean_accuracy = np.nanmean(accuracies)\n",
    "    return mean_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aa77a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def price(best_model_params, dir_path, iteration, plot):\n",
    "    #Gets the predictions of the highest overall IC in the boosted round cases\n",
    "    if iteration == False:\n",
    "        best_prediction=best_model_params['predictions'][[str(extract_number(best_model_params['metrics'].loc[:, best_model_params['metrics'].columns.str.startswith(\"daily_metric\")].idxmax(axis=1)[0])), 'i']]\n",
    "    else:\n",
    "        best_prediction=best_model_params['predictions'][[str(iteration), 'i']]\n",
    "    actual_return=best_model_params['returns']\n",
    "    #Merge actual returns and prediction returns\n",
    "    merged = pd.merge(best_prediction, actual_return, left_index=True, right_index=True, how='left')\n",
    "    merged.columns =['predictions', 'window', 'returns']\n",
    "    merged.window = merged.window.astype(int)\n",
    "    #Shift actual returns 1 day back\n",
    "    merged['returns'] = merged.groupby('permno')['returns'].shift(-1)\n",
    "    merged = merged.dropna()\n",
    "\n",
    "    if plot == False:\n",
    "        return merged\n",
    "        \n",
    "    print('Best num_iterations: ' + str(best_prediction.columns[0]))\n",
    "    print(f\"Neutral Accuracy: {round(sign_accuracy(merged.predictions, merged.returns, None, 'price'), 2)}%\")\n",
    "    print(f\"Positive Accuracy: {round(sign_accuracy(merged.predictions, merged.returns, 'positive', 'price'), 2)}%\")\n",
    "    print(f\"Negative Accuracy: {round(sign_accuracy(merged.predictions, merged.returns, 'negative', 'price'), 2)}%\")\n",
    "    display(best_model_params['metrics'])\n",
    "    \n",
    "    # Convert to HTML\n",
    "    df_html = best_model_params['metrics'].to_html(classes='my-table')\n",
    "\n",
    "    # Prepare the plot\n",
    "    plt.style.use('ggplot')\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    best_model_params['daily_metric'][best_prediction.columns[0]].rolling(window=42).mean().plot(\n",
    "        ax=ax, linewidth=0.5, color='blue', linestyle='-', title='Daily Metric Plot'\n",
    "    )\n",
    "    ax.set(xlabel='Date', ylabel='Daily Metric')\n",
    "    ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Convert plot to SVG\n",
    "    img_stream = BytesIO()\n",
    "    fig.savefig(img_stream, format='png')\n",
    "    img_base64 = base64.b64encode(img_stream.getvalue()).decode()\n",
    "\n",
    "    with open(dir_path / 'metric.html', 'w') as f:\n",
    "        f.write('''<!DOCTYPE html>\n",
    "        <html lang=\"en\">\n",
    "        <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Metrics Report</title>\n",
    "        <style>\n",
    "            body {\n",
    "                font-family: 'Times New Roman', serif;\n",
    "                margin: 0;\n",
    "                padding: 0;\n",
    "                background-color: #ffffff;\n",
    "                text-align: center;\n",
    "                color: #000000;\n",
    "            }\n",
    "            header {\n",
    "                background: #d3d3d3;\n",
    "                color: #000000;\n",
    "                text-align: center;\n",
    "                padding-top: 30px;\n",
    "                min-height: 70px;\n",
    "                margin: 0 auto;\n",
    "            }\n",
    "            .container {\n",
    "                width: 70%;\n",
    "                margin: auto;\n",
    "            }\n",
    "            .main-content {\n",
    "                padding: 30px;\n",
    "            }\n",
    "            .my-table {\n",
    "                width: 100%; \n",
    "                border-collapse: collapse;\n",
    "                font-size: 12px;\n",
    "                table-layout: fixed; \n",
    "            }\n",
    "            th, td {\n",
    "                border: 2px solid #e3e3e3;\n",
    "                padding: 10px;\n",
    "                text-align: left;\n",
    "                word-wrap: break-word;\n",
    "            }\n",
    "        </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <header>\n",
    "                <h1>Metrics Report</h1>\n",
    "            </header>\n",
    "            <div class=\"container\">\n",
    "                <div class=\"main-content\">\n",
    "                    <p>Best num_iterations: ''' + str(best_prediction.columns[0]) + '''</p>\n",
    "                    <p>Neutral Accuracy: ''' + f\"{round(sign_accuracy(merged.predictions, merged.returns, None, 'price'), 2)}%\" + '''</p>\n",
    "                    <p>Positive Accuracy: ''' + f\"{round(sign_accuracy(merged.predictions, merged.returns, 'positive', 'price'), 2)}%\" + '''</p>\n",
    "                    <p>Negative Accuracy: ''' + f\"{round(sign_accuracy(merged.predictions, merged.returns, 'negative', 'price'), 2)}%\" + '''</p>\n",
    "                    ''' + df_html + '''\n",
    "                    <img src=\"data:image/png;base64,''' + img_base64 + '''\" alt=\"plot\" />\n",
    "                </div>\n",
    "            </div>\n",
    "        </body>\n",
    "        </html>''')\n",
    "    return merged\n",
    "\n",
    "def plot_ensemble(merged, ic_by_day):\n",
    "    print(f'Daily IC Mean: {round(ic_by_day.mean()[0], 5)}')\n",
    "    # Prepare the plot\n",
    "    plt.style.use('ggplot')\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ic_by_day.rolling(window=42).mean().plot(ax=ax, linewidth=0.5, color='blue', linestyle='-', title='Daily Metric Plot')\n",
    "    ax.set(xlabel='Date', ylabel='Daily Metric')\n",
    "    ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Convert plot to SVG\n",
    "    img_stream = BytesIO()\n",
    "    fig.savefig(img_stream, format='png')\n",
    "    img_base64 = base64.b64encode(img_stream.getvalue()).decode()\n",
    "    \n",
    "    with open(dir_path / 'metric.html', 'w') as f:\n",
    "        f.write('''<!DOCTYPE html>\n",
    "        <html lang=\"en\">\n",
    "        <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Metrics Report</title>\n",
    "        <style>\n",
    "            body {\n",
    "                font-family: 'Times New Roman', serif;\n",
    "                margin: 0;\n",
    "                padding: 0;\n",
    "                background-color: #ffffff;\n",
    "                text-align: center;\n",
    "                color: #000000;\n",
    "            }\n",
    "            header {\n",
    "                background: #d3d3d3;\n",
    "                color: #000000;\n",
    "                text-align: center;\n",
    "                padding-top: 30px;\n",
    "                min-height: 70px;\n",
    "                margin: 0 auto;\n",
    "            }\n",
    "            .container {\n",
    "                display: flex;\n",
    "                justify-content: center;\n",
    "                align-items: center;\n",
    "                flex-direction: column;\n",
    "            }\n",
    "            .main-content {\n",
    "                padding: 30px;\n",
    "            }\n",
    "            .my-table {\n",
    "                width: 100%; \n",
    "                border-collapse: collapse;\n",
    "                font-size: 12px;\n",
    "                table-layout: fixed; \n",
    "            }\n",
    "            th, td {\n",
    "                border: 2px solid #e3e3e3;\n",
    "                padding: 10px;\n",
    "                text-align: left;\n",
    "                word-wrap: break-word;\n",
    "            }\n",
    "        </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <header>\n",
    "                <h1>Metrics Report</h1>\n",
    "            </header>\n",
    "            <div class=\"container\">\n",
    "                <div class=\"main-content\">\n",
    "                    <p>Neutral Accuracy: ''' + f\"{round(sign_accuracy(merged.predictions, merged.returns, None, 'price'), 2)}%\" + '''</p>\n",
    "                    <p>Positive Accuracy: ''' + f\"{round(sign_accuracy(merged.predictions, merged.returns, 'positive', 'price'), 2)}%\" + '''</p>\n",
    "                    <p>Negative Accuracy: ''' + f\"{round(sign_accuracy(merged.predictions, merged.returns, 'negative', 'price'), 2)}%\" + '''</p>\n",
    "                    <img src=\"data:image/png;base64,''' + img_base64 + '''\" alt=\"plot\" />\n",
    "                </div>\n",
    "            </div>\n",
    "        </body>\n",
    "        </html>''')\n",
    "\n",
    "def sign(best_model_params):\n",
    "    #Gets the predictions of the highest overall IC in the boosted round cases\n",
    "    best_prediction=best_model_params['predictions'][str(extract_number(best_model_params['metrics'].loc[:, best_model_params['metrics'].columns.str.startswith(\"daily_metric\")].idxmax(axis=1)[0]))].to_frame()\n",
    "    actual_return=best_model_params['returns']\n",
    "    #Merge actual returns and prediction returns\n",
    "    merged = pd.merge(best_prediction, actual_return, left_index=True, right_index=True, how='left')\n",
    "    merged.columns =['predictions', 'returns']\n",
    "    #Shift actual returns 1 day back\n",
    "    merged['returns'] = merged.groupby('permno')['returns'].shift(-1)\n",
    "    merged = merged.dropna()\n",
    "    print('Best num_iterations: ' + str(best_prediction.columns[0]))\n",
    "    print(f\"Neutral Accuracy: {round(sign_accuracy(merged.predictions, merged.returns, None, 'sign'), 2)}%\")\n",
    "    print(f\"Positive Accuracy: {round(sign_accuracy(merged.predictions, merged.returns, 'positive', 'sign'), 2)}%\")\n",
    "    print(f\"Negative Accuracy: {round(sign_accuracy(merged.predictions, merged.returns, 'negative', 'sign'), 2)}%\")\n",
    "    display(best_model_params['metrics'])\n",
    "    return merged\n",
    "\n",
    "def gain(gain):\n",
    "    # Sort by mean gain in descending order\n",
    "    sorted_df = gain.sort_values(by='mean', ascending=False)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(15, 50))\n",
    "    sorted_df['mean'].plot(kind='barh')\n",
    "    plt.xlabel('Average Gain')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title('Feature Importances based on Gain')\n",
    "    plt.gca().invert_yaxis()  # to have the most important feature on top\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def split(split):\n",
    "    # Sort by mean gain in descending order\n",
    "    sorted_df = split.sort_values(by='mean', ascending=False)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(15, 50))\n",
    "    sorted_df['mean'].plot(kind='barh')\n",
    "    plt.xlabel('Average Split')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title('Feature Importances based on Split')\n",
    "    plt.gca().invert_yaxis()  # to have the most important feature on top\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f53aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hrp(returns, stocks, date):\n",
    "    def correl_dist(corr):\n",
    "        # A distance matrix based on correlation, where 0<=d[i,j]<=1\n",
    "        # This is a proper distance metric\n",
    "        dist = ((1 - corr) / 2.)**.5  # distance matrix\n",
    "        dist = squareform(dist)\n",
    "        return dist\n",
    "\n",
    "    def get_quasi_diag(link):\n",
    "        # Sort clustered items by distance\n",
    "        link = link.astype(int)\n",
    "        sortIx = pd.Series([link[-1, 0], link[-1, 1]])\n",
    "        numItems = link[-1, 3]  # number of original items\n",
    "        while sortIx.max() >= numItems:\n",
    "            sortIx.index = range(0, sortIx.shape[0] * 2, 2)  # make space\n",
    "            df0 = sortIx[sortIx >= numItems]  # find clusters\n",
    "            i = df0.index\n",
    "            j = df0.values - numItems\n",
    "            sortIx[i] = link[j, 0]  # item 1\n",
    "            df1 = pd.Series(link[j, 1], index=(i + 1).tolist())\n",
    "            sortIx = pd.concat([sortIx, df1]).sort_index()\n",
    "            sortIx.index = range(sortIx.shape[0])\n",
    "        return sortIx.tolist()\n",
    "\n",
    "    def get_cluster_var(cov, cItems):\n",
    "        # Compute variance per cluster\n",
    "        cov_= cov.loc[cItems,cItems] # matrix slice\n",
    "        # Compute the inverse_variance portfolio\n",
    "        ivp = 1. / np.diag(cov_)\n",
    "        ivp /= ivp.sum()\n",
    "        w_= ivp.reshape(-1,1)\n",
    "        cVar = np.dot(np.dot(w_.T,cov_),w_)[0,0]\n",
    "        return cVar\n",
    "\n",
    "    def get_rec_bipart(cov, sortIx):\n",
    "        # Compute HRP alloc\n",
    "        w = pd.Series(1, index=sortIx)\n",
    "        cItems = [sortIx]  # initialize all items in one cluster\n",
    "        while len(cItems) > 0:\n",
    "            cItems = [i[j:k] for i in cItems for j, k in ((0, len(i) // 2), (len(i) // 2, len(i))) if len(i) > 1]  # bi-section\n",
    "            for i in range(0, len(cItems), 2):  # parse in pairs\n",
    "                cItems0 = cItems[i]  # cluster 1\n",
    "                cItems1 = cItems[i + 1]  # cluster 2\n",
    "                cVar0 = get_cluster_var(cov, cItems0)\n",
    "                cVar1 = get_cluster_var(cov, cItems1)\n",
    "                alpha = 1 - cVar0 / (cVar0 + cVar1)\n",
    "                w[cItems0] *= alpha  # weight 1\n",
    "                w[cItems1] *= 1 - alpha  # weight 2\n",
    "        return w\n",
    "    \n",
    "    cov, corr = returns.cov(), returns.corr()\n",
    "    # Construct a hierarchical portfolio\n",
    "    dist = correl_dist(corr)\n",
    "    link = sch.linkage(dist, 'single')\n",
    "    sortIx = get_quasi_diag(link)\n",
    "    sortIx = corr.index[sortIx].tolist()\n",
    "    hrp = get_rec_bipart(cov, sortIx)\n",
    "    return hrp.sort_index().values\n",
    "\n",
    "def ivp(returns, stocks, date):\n",
    "    # Compute the inverse-variance portfolio\n",
    "    cov = returns.cov()\n",
    "    ivp = 1. / np.diag(cov)\n",
    "    ivp /= ivp.sum()\n",
    "    return ivp\n",
    "\n",
    "def ewp(returns, stocks, date, leverage):\n",
    "    weight = leverage/len(returns.columns)\n",
    "    return np.full(len(returns.columns), weight)\n",
    "\n",
    "def mvp(returns, stocks, date):\n",
    "    cov = returns.cov()\n",
    "    cov = cov.T.values\n",
    "    n = len(cov)\n",
    "    N = 100\n",
    "    mus = [10 ** (5.0 * t / N - 1.0) for t in range(N)]\n",
    "\n",
    "    # Convert to cvxopt matrices\n",
    "    S = opt.matrix(cov)\n",
    "    #pbar = opt.matrix(np.mean(returns, axis=1))\n",
    "    pbar = opt.matrix(np.ones(cov.shape[0]))\n",
    "\n",
    "    # Create constraint matrices\n",
    "    G = -opt.matrix(np.eye(n))  # negative n x n identity matrix\n",
    "    h = opt.matrix(0.0, (n, 1))\n",
    "    A = opt.matrix(1.0, (1, n))\n",
    "    b = opt.matrix(1.0)\n",
    "    \n",
    "    # Calculate efficient frontier weights using quadratic programming\n",
    "    solvers.options['show_progress'] = False\n",
    "    portfolios = [solvers.qp(mu * S, -pbar, G, h, A, b)['x'] for mu in mus]\n",
    "    ## CALCULATE RISKS AND RETURNS FOR FRONTIER    \n",
    "    returns = [blas.dot(pbar, x) for x in portfolios]\n",
    "    risks = [np.sqrt(blas.dot(x, S * x)) for x in portfolios]\n",
    "    ## CALCULATE THE 2ND DEGREE POLYNOMIAL OF THE FRONTIER CURVE\n",
    "    m1 = np.polyfit(returns, risks, 2)\n",
    "    x1 = np.sqrt(m1[2] / m1[0])\n",
    "    # CALCULATE THE OPTIMAL PORTFOLIO    \n",
    "    wt = solvers.qp(opt.matrix(x1 * S), -pbar, G, h, A, b)['x']\n",
    "    return np.asarray(list(wt))\n",
    "\n",
    "# Minimize Factor Loading Optimization\n",
    "def mfl(data, type, sb, factor_weights):\n",
    "    def objective_cvxpy(weights, betas, factor_weights):\n",
    "        # Initialize the alternative objective\n",
    "        alternative_objective = 0.0\n",
    "    \n",
    "        # Loop through each factor and calculate its contribution to the objective\n",
    "        for factor, weight in factor_weights.items():\n",
    "            port_beta = cp.matmul(weights, betas[factor])\n",
    "            alternative_objective += weight * cp.square(port_beta)\n",
    "    \n",
    "        return alternative_objective\n",
    "        \n",
    "    collect_weight = []\n",
    "    count = 0 \n",
    "    print(f'Processing {type}...')\n",
    "    for date, row in data.iterrows():\n",
    "        # Get betas for given stocks at day t\n",
    "        betas = sb.loc[sb.index.get_level_values(1) == date]\n",
    "        stocks = row[type]\n",
    "        betas = get_stocks_data(betas, stocks)\n",
    "        betas = betas.fillna(0)\n",
    "\n",
    "        # CVXPY setup\n",
    "        weights = cp.Variable(betas.shape[0])\n",
    "        objective = cp.Minimize(objective_cvxpy(weights, betas, factor_weights))\n",
    "        constraints = [cp.sum(weights) == 1, weights >= 0, weights <= 1]\n",
    "        prob = cp.Problem(objective, constraints)\n",
    "        \n",
    "        # Solve the problem\n",
    "        # prob.solve()\n",
    "        prob.solve(solver='ECOS')\n",
    "        collect_weight.append(weights.value)\n",
    "        \n",
    "        # Track performance\n",
    "        if count % 30 == 0:\n",
    "            print(date)\n",
    "        count += 1\n",
    "    \n",
    "    return collect_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012fe331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_period(period, period_returns, num_stocks, candidates, threshold):\n",
    "    # Find sp500 candidates for the given year and assign it to data\n",
    "    period_year = period.index.get_level_values('date')[0].year\n",
    "    sp500 = candidates[period_year]\n",
    "    tickers = common_stocks(sp500, period)\n",
    "    sp500_period = get_stocks_data(period, tickers)\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(period_year)\n",
    "    print('Num of tickers: ' + str(len(tickers)))\n",
    "\n",
    "    # Filter the DataFrame to only include rows with market cap over the threshold\n",
    "    filtered_period = period[period['market_cap'] > threshold]\n",
    "        \n",
    "    # Group by date and compute long and short stocks and their returns\n",
    "    for date, stocks in filtered_period.groupby('date'):\n",
    "        sorted_stocks = stocks.sort_values(by='predictions')\n",
    "        long_stocks = sorted_stocks.index.get_level_values('ticker')[-num_stocks:]\n",
    "        short_stocks = sorted_stocks.index.get_level_values('ticker')[:num_stocks]\n",
    "        \n",
    "        # Store results in period_returns DataFrame\n",
    "        period_returns.loc[date] = [long_stocks.tolist(), sorted_stocks.iloc[-num_stocks:].returns.values,\n",
    "                                    short_stocks.tolist(), sorted_stocks.iloc[:num_stocks].returns.values]\n",
    "\n",
    "def backtest(data, num_stocks, threshold):\n",
    "    global live\n",
    "    # Set portfolio weights and other tracking variables\n",
    "    period_returns = pd.DataFrame(columns=['longStocks', 'longRet', 'shortStocks', 'shortRet'])\n",
    "    \n",
    "    # Get candidates\n",
    "    candidates = get_candidate(live)\n",
    "    \n",
    "    # Loop over each group in tic.groupby('window')\n",
    "    for _, df in tic.groupby('window'):\n",
    "        df = df.reset_index().set_index(['ticker', 'date']).drop('window', axis=1)\n",
    "        process_period(df, period_returns, num_stocks, candidates, threshold)\n",
    "    \n",
    "    return period_returns\n",
    "\n",
    "def plot_hist(data, date):\n",
    "    pred = data.predictions.loc[data.index.get_level_values('date')==date]\n",
    "    ret = data.returns.loc[data.index.get_level_values('date')==date]\n",
    "    plt.hist(pred, bins='auto', edgecolor='black', alpha=0.5, label=f\"Pred: {date}\")\n",
    "    plt.hist(ret, bins='auto', edgecolor='red', alpha=0.5, label=f\"Real: {date}\")\n",
    "    plt.title('Histogram of values')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41d52ce-63eb-4d48-90b0-9d67525f09ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_ret(best_model_params, iteration):\n",
    "    #Gets the predictions of the highest overall IC in the boosted round cases\n",
    "    if iteration == False:\n",
    "        best_prediction=best_model_params['predictions'][[str(extract_number(best_model_params['metrics'].loc[:, best_model_params['metrics'].columns.str.startswith(\"daily_metric\")].idxmax(axis=1)[0])), 'i']]\n",
    "    else:\n",
    "        best_prediction=best_model_params['predictions'][[str(iteration), 'i']]\n",
    "    actual_return=best_model_params['returns']\n",
    "    #Merge actual returns and prediction returns\n",
    "    merged = pd.merge(best_prediction, actual_return, left_index=True, right_index=True, how='left')\n",
    "    merged.columns =['predictions', 'window', 'returns']\n",
    "    merged.window = merged.window.astype(int)\n",
    "    #Shift actual returns 1 day back\n",
    "    merged['returns'] = merged.groupby('permno')['returns'].shift(-1)\n",
    "    merged = merged.dropna()\n",
    "    return merged\n",
    "\n",
    "def sharpe_process_period(period, period_returns, num_stocks, candidates, threshold):\n",
    "    # Find sp500 candidates for the given year and assign it to data\n",
    "    period_year = period.index.get_level_values('date')[0].year\n",
    "    # sp500 = candidates[period_year]\n",
    "    # tickers = common_stocks(sp500, period)\n",
    "    # sp500_period = get_stocks_data(period, tickers)\n",
    "\n",
    "    # Filter the DataFrame to only include rows with market cap over the threshold\n",
    "    filtered_period = period[period['market_cap'] > threshold]\n",
    "\n",
    "    # Group by date and compute long and short stocks and their returns\n",
    "    for date, stocks in filtered_period.groupby('date'):\n",
    "        sorted_stocks = stocks.sort_values(by='predictions')\n",
    "        long_stocks = sorted_stocks.index.get_level_values('permno')[-num_stocks:]\n",
    "        short_stocks = sorted_stocks.index.get_level_values('permno')[:num_stocks]\n",
    "        \n",
    "        # Store results in period_returns DataFrame\n",
    "        period_returns.loc[date] = [long_stocks.tolist(), sorted_stocks.iloc[-num_stocks:].returns.values,\n",
    "                                    short_stocks.tolist(), sorted_stocks.iloc[:num_stocks].returns.values]\n",
    "\n",
    "\n",
    "def sharpe_backtest(data, num_stocks, threshold):\n",
    "    global live\n",
    "    # Set portfolio weights and other tracking variables\n",
    "    period_returns = pd.DataFrame(columns=['longStocks', 'longRet', 'shortStocks', 'shortRet'])\n",
    "    \n",
    "    # Get candidates\n",
    "    candidates = get_candidate(live)\n",
    "    \n",
    "    # Loop over each group in tic.groupby('window')\n",
    "    for _, df in tic.groupby('window'):\n",
    "        df = df.reset_index().set_index(['permno', 'date']).drop('window', axis=1)\n",
    "        sharpe_process_period(df, period_returns, num_stocks, candidates, threshold)\n",
    "    \n",
    "    return period_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aa5141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def port_opt(data, window_size, port_opt_func, option, leverage, file=None, factor_weights=None, factor_cols=None):\n",
    "    global live\n",
    "    assert port_opt_func==ewp or port_opt_func==hrp or port_opt_func==mvp or port_opt_func==ivp or port_opt_func==mfl, 'Must use callable function ewp, hrp, mvp, ivp, or mfl'\n",
    "    if port_opt_func==hrp or port_opt_func==mvp or port_opt_func==ivp or port_opt_func==mfl:\n",
    "        assert window_size>0, 'Window size must be greater than 0 if using hrp, mvp, ivp, or mfl'\n",
    "    @ray.remote\n",
    "    def exec_port_opt(i, data, window_size, port_opt_func, option):\n",
    "        if option == 'long':  \n",
    "            # Get window data for long returns\n",
    "            long_returns = pd.DataFrame(data['longRet'][i:i+window_size].tolist())\n",
    "            long_stocks = data['longStocks'][i:i+window_size][-1]\n",
    "            date = data.iloc[i:i+window_size].index[-1]\n",
    "            # Perform desired portfolio optimization\n",
    "            long_weights = port_opt_func(long_returns, long_stocks, date)\n",
    "            total_ret = np.sum(long_returns.iloc[-1] * long_weights)\n",
    "        elif option == 'short':\n",
    "            # Get window data for short returns\n",
    "            short_returns = -1 * pd.DataFrame(data['shortRet'][i:i+window_size].tolist())\n",
    "            short_stocks = data['shortStocks'][i:i+window_size][-1]\n",
    "            date = data.iloc[i:i+window_size].index[-1]\n",
    "            # Perform desired portfolio optimization\n",
    "            short_weights = port_opt_func(short_returns, short_stocks, date)\n",
    "            total_ret = np.sum(short_returns.iloc[-1] * short_weights)\n",
    "        elif option == 'both':\n",
    "            # Get window data for long returns and short returns\n",
    "            # Perform desired portfolio optimization\n",
    "            date = data.iloc[i:i+window_size].index[-1]\n",
    "            long_returns = pd.DataFrame(data['longRet'][i:i+window_size].tolist())\n",
    "            long_stocks = data['longStocks'][i:i+window_size][-1]\n",
    "            long_weights = port_opt_func(long_returns, long_stocks, date)\n",
    "            \n",
    "            short_returns = -1 * pd.DataFrame(data['shortRet'][i:i+window_size].tolist())\n",
    "            short_stocks = data['shortStocks'][i:i+window_size][-1]\n",
    "            short_weights = port_opt_func(short_returns, short_stocks, date)\n",
    "            total_ret = np.sum(long_returns.iloc[-1] * long_weights) + np.sum(short_returns.iloc[-1] * short_weights)\n",
    "            \n",
    "        return total_ret\n",
    "    \n",
    "    if port_opt_func == ewp:\n",
    "        if option == 'long':\n",
    "            # Get long returns\n",
    "            long_returns = pd.DataFrame(data['longRet'].tolist())\n",
    "            long_weights = port_opt_func(long_returns, None, None, leverage)\n",
    "\n",
    "            # Calculate equal-weights and sum up to get total strategy return\n",
    "            long_weights = np.tile(long_weights, (len(long_returns), 1))\n",
    "            total_ret = np.sum(long_returns.values * long_weights, axis=1)\n",
    "        elif option == 'short':\n",
    "            # Get short returns\n",
    "            short_returns = -1 *pd.DataFrame(data['shortRet'].tolist())\n",
    "            short_weights = port_opt_func(short_returns, None, None, leverage)\n",
    "\n",
    "            # Calculate equal-weights and sum up to get total strategy return\n",
    "            short_weights = np.tile(short_weights, (len(short_returns), 1))\n",
    "            total_ret = np.sum(short_returns.values * short_weights, axis=1)\n",
    "        elif option == 'both':\n",
    "            # Get long returns\n",
    "            long_returns = pd.DataFrame(data['longRet'].tolist())\n",
    "            long_weights = port_opt_func(long_returns, None, None, leverage)\n",
    "            long_weights = np.tile(long_weights, (len(long_returns), 1))\n",
    "\n",
    "            # Get short returns\n",
    "            short_returns = -1 * pd.DataFrame(data['shortRet'].tolist())\n",
    "            short_weights = port_opt_func(short_returns, None, None, leverage)\n",
    "            short_weights = np.tile(short_weights, (len(short_returns), 1))\n",
    "\n",
    "            # Calculate equal-weights and sum up to get total strategy return\n",
    "            total_ret = np.sum(long_returns.values * long_weights, axis=1) + np.sum(short_returns.values * short_weights, axis=1)\n",
    "        data['totalRet'] = total_ret\n",
    "        return data\n",
    "\n",
    "    if port_opt_func == mfl:\n",
    "        # Read in calculate betas and convert from permno to ticker index\n",
    "        stock = read_stock(get_load_data_large_dir() / 'permno_to_train_fund.csv')\n",
    "        sb = ModelPrep(factor_name=file, group='permno', interval='D', kind='price', stock=stock, div=False, start='2006-01-01', end='2023-01-01', save=False).prep()\n",
    "        ticker = pd.read_parquet(get_parquet_dir(live) / 'data_ticker.parquet.brotli')\n",
    "        sb = sb.merge(ticker, left_index=True, right_index=True, how='left')\n",
    "        sb = sb.reset_index().set_index(['ticker', 'date'])\n",
    "        sb = sb.sort_index(level=['ticker', 'date'])\n",
    "        sb = sb.drop('permno', axis=1)\n",
    "        sb = sb[factor_cols]\n",
    "        # For some reason, reseting index and setting it to ticker, date creates duplicate indices (should probably check this out)\n",
    "        sb = sb[~sb.index.duplicated(keep='first')]\n",
    "        \n",
    "        if option == 'long':\n",
    "            # Calculate minimized-factor loading weights and sum it up to get strategy return\n",
    "            long_weights = mfl(data, 'longStocks', sb, factor_weights)\n",
    "            long_returns = pd.DataFrame(data['longRet'].tolist())\n",
    "            long_weights_df = pd.DataFrame(long_weights)\n",
    "            total_ret = np.sum(long_returns.values * long_weights_df.values, axis=1)\n",
    "        elif option == 'short':\n",
    "            # Calculate minimized-factor loading weights and sum it up to get strategy return\n",
    "            short_weights = mfl(data, 'shortStocks', sb, factor_weights)\n",
    "            short_returns = -1 *pd.DataFrame(data['shortRet'].tolist())\n",
    "            short_weights_df = pd.DataFrame(short_weights)\n",
    "            total_ret = np.sum(short_returns.values * short_weights_df.values, axis=1)\n",
    "        elif option == 'both':\n",
    "            # Get long returns\n",
    "            long_weights = mfl(data, 'longStocks', sb, factor_weights)\n",
    "            long_returns = pd.DataFrame(data['longRet'].tolist())\n",
    "\n",
    "            # Get short returns\n",
    "            short_weights = mfl(data, 'shortStocks', sb, factor_weights)\n",
    "            short_returns = -1 *pd.DataFrame(data['shortRet'].tolist())\n",
    "\n",
    "            # Calculate minimized-factor loading weights and sum it up to get strategy return\n",
    "            long_weights_df = pd.DataFrame(long_weights)\n",
    "            short_weights_df = pd.DataFrame(short_weights)\n",
    "            total_ret = np.sum(long_returns.values * long_weights_df.values, axis=1) + np.sum(short_returns.values * short_weights_df.values, axis=1)\n",
    "        data['totalRet'] = total_ret\n",
    "        return data\n",
    "        \n",
    "    ray.init(num_cpus=16, ignore_reinit_error=True)\n",
    "    start_time = time.time()\n",
    "    total_ret_collect = ray.get([exec_port_opt.remote(i, data, window_size, port_opt_func, option) for i in range(0, len(data) - window_size + 1)])\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    rounded_time = round(elapsed_time, 2)\n",
    "    print(f\"Elapsed time: {rounded_time} seconds\")\n",
    "    ray.shutdown()\n",
    "    \n",
    "    data = data[window_size-1:]\n",
    "    data['totalRet'] = total_ret_collect\n",
    "    return data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6316c41f",
   "metadata": {},
   "source": [
    "# Read in Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af1b8a0-25ba-4ef5-b7dd-8f8f227b4564",
   "metadata": {},
   "outputs": [],
   "source": [
    "live = False\n",
    "trial = 'lightGBM_trial_231'\n",
    "files = read_result(trial, 'metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3828859-a9a6-4cea-9557-67f3b00cd7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "live = True\n",
    "format_end = date.today().strftime('%Y%m%d')\n",
    "model_name = f'lightgbm_{format_end}'\n",
    "files = read_result(model_name, 'metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5a353b-3ce2-4c2d-8d93-9c19cab403e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b//|"
     ]
    }
   ],
   "source": [
    "keep = {}\n",
    "ticker = pd.read_parquet(get_parquet(live) / 'data_ticker.parquet.brotli')\n",
    "misc = pd.read_parquet(get_parquet(live) / 'data_misc.parquet.brotli', columns=['market_cap'])\n",
    "for i, row in files.iterrows():\n",
    "    read_file = get_max_ic_file(row, trial)\n",
    "    returns = sharpe_ret(read_file, iteration=False)\n",
    "    # Convert Permno to Ticker\n",
    "    tic = returns.merge(ticker, left_index=True, right_index=True, how='left')\n",
    "    tic = tic.merge(misc, left_index=True, right_index=True, how='left')\n",
    "    tic = tic.reset_index().set_index(['window', 'permno', 'date'])\n",
    "    pred = sharpe_backtest(tic, num_stocks=20, threshold=10_000_000_000)\n",
    "    equal_weight = port_opt(data=pred, window_size=21, port_opt_func=ewp, option='both', leverage=0.5)\n",
    "    short_weight = port_opt(data=pred, window_size=21, port_opt_func=ewp, option='short', leverage=1)['totalRet']\n",
    "    long_weight = port_opt(data=pred, window_size=21, port_opt_func=ewp, option='long', leverage=1)['totalRet']\n",
    "    stock = equal_weight['totalRet']\n",
    "    sharpe = qs.stats.sharpe(stock)    \n",
    "    calmar = qs.stats.calmar(stock)\n",
    "    print('-' * 60)\n",
    "    print(f'Row: {i}')\n",
    "    display(read_file['metrics'])\n",
    "    print(f'SHARPE Ratio: {sharpe}')\n",
    "    print(f'CALMAR Ratio: {calmar}')\n",
    "    keep[i] = sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39ad720-8651-45b6-877a-3f9ac5659e53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qs.reports.full(long_weight, 'SPY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf8129-8012-466e-9169-855fff2b16d0",
   "metadata": {},
   "source": [
    "# Minimizing Market Exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6184df30-d709-48d9-a82d-5c0e848b8ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfl(data, stocks_to_use, col_name, exp, min_exp_weights, lambda_reg):\n",
    "    def objective_cvxpy(weights, betas, min_exp_weights, lambda_reg):\n",
    "        # Initialize the alternative objective\n",
    "        factor_exposure  = 0.0\n",
    "        # Loop through each factor and calculate its contribution to the objective\n",
    "        for factor, weight in min_exp_weights.items():\n",
    "            port_beta = cp.matmul(weights, betas[factor])\n",
    "            factor_exposure += weight * cp.square(port_beta)\n",
    "            \n",
    "        l2_reg = cp.norm(weights, 2)**2\n",
    "        return factor_exposure + lambda_reg * l2_reg\n",
    "\n",
    "    collect_weight = []\n",
    "    df_collect = []\n",
    "    count = 0 \n",
    "    print(f'Processing {type}...')\n",
    "    for date, row in stocks_to_use.iterrows():\n",
    "        stocks = stocks_to_use.loc[stocks_to_use.index == date][col_name][0]\n",
    "        # Get betas for given stocks at day t\n",
    "        betas = exp.loc[exp.index.get_level_values('date') == date]\n",
    "        betas = get_stocks_data(betas, stocks)\n",
    "        betas = betas.sort_index(level=['permno', 'date'])\n",
    "        betas = betas.fillna(0)\n",
    "\n",
    "        # CVXPY setup\n",
    "        weights = cp.Variable(betas.shape[0])\n",
    "        objective = cp.Minimize(objective_cvxpy(weights, betas, min_exp_weights, lambda_reg))\n",
    "        constraints = [\n",
    "            cp.sum(weights) == 1,\n",
    "            weights >= 0.001,\n",
    "            weights <= 0.10\n",
    "        ]\n",
    "        prob = cp.Problem(objective, constraints)\n",
    "        prob.solve(solver='ECOS')\n",
    "        collect_weight.append(weights.value)\n",
    "        df_collect.append(betas.drop(betas.columns, axis=1))\n",
    "        \n",
    "        # Track performance\n",
    "        if count % 30 == 0:\n",
    "            print(date)\n",
    "        count += 1\n",
    "\n",
    "    # Create Dataframe\n",
    "    save_df = pd.concat(df_collect, axis=0)\n",
    "    # Flatten List\n",
    "    flattened_array = np.concatenate(collect_weight).flatten()\n",
    "    flattened_weight = flattened_array.tolist()\n",
    "    # Add list to column\n",
    "    save_df['exp_weight'] = flattened_weight\n",
    "    return save_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9338db34-1477-4924-b3f0-1353abdb445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfl(data, stocks_to_use, col_name, exp, min_exp_weights, lambda_reg):\n",
    "    def objective_cvxpy(weights, betas, min_exp_weights, lambda_reg):\n",
    "        factor_exposure = 0.0\n",
    "        for factor, weight in min_exp_weights.items():\n",
    "            port_beta = cp.matmul(weights, betas[factor])\n",
    "            factor_exposure += weight * cp.square(port_beta)\n",
    "        l2_reg = cp.norm(weights, 2)**2\n",
    "        return factor_exposure + lambda_reg * l2_reg\n",
    "\n",
    "    collect_weight = []\n",
    "    df_collect = []\n",
    "    count = 0\n",
    "    print(f'Processing {type}...')\n",
    "    for date, row in stocks_to_use.iterrows():\n",
    "        stocks = stocks_to_use.loc[stocks_to_use.index == date][col_name][0]\n",
    "        betas = exp.loc[exp.index.get_level_values('date') == date]\n",
    "        betas = get_stocks_data(betas, stocks)\n",
    "        betas = betas.sort_index(level=['permno', 'date'])\n",
    "        betas = betas.fillna(0)\n",
    "\n",
    "        # CVXPY setup\n",
    "        weights = cp.Variable(betas.shape[0])\n",
    "        total_beta = cp.matmul(betas.to_numpy().T, weights)\n",
    "            \n",
    "        objective = cp.Minimize(objective_cvxpy(weights, betas, min_exp_weights, lambda_reg))\n",
    "        constraints = [\n",
    "            cp.sum(weights) == 1,\n",
    "            weights >= 0.001,\n",
    "            weights <= 0.10\n",
    "        ]\n",
    "        prob = cp.Problem(objective, constraints)\n",
    "        prob.solve(solver='ECOS')\n",
    "        collect_weight.append(weights.value)\n",
    "        df_collect.append(betas.drop(betas.columns, axis=1))\n",
    "        \n",
    "        if count % 30 == 0:\n",
    "            print(date)\n",
    "        count += 1\n",
    "\n",
    "    # Create Dataframe\n",
    "    save_df = pd.concat(df_collect, axis=0)\n",
    "    # Flatten List\n",
    "    flattened_array = np.concatenate(collect_weight).flatten()\n",
    "    flattened_weight = flattened_array.tolist()\n",
    "    # Add list to column\n",
    "    save_df['exp_weight'] = flattened_weight\n",
    "    return save_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f59648-6000-42a5-b53f-d959ef7698fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected Return\n",
    "returns['RET_01_expected'] = returns.groupby('permno')['returns'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cfe0c2-89ee-4538-b38d-d50decc5365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = read_stock(get_large(live) / 'permno_to_train_fund.csv')\n",
    "sb_sector = ModelPrep(live=False, factor_name='factor_sb_sector', group='permno', interval='D', kind='price', stock=stock, div=False, start='2014-01-01', end='2024-01-01', save=False).prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a3042e-535e-493c-8fa2-153dcf4affe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_col = ['XLB_RET_01_sector_01_126', 'XLE_RET_01_sector_01_126', 'XLF_RET_01_sector_01_126', 'XLI_RET_01_sector_01_126',\n",
    "              'XLK_RET_01_sector_01_126', 'XLP_RET_01_sector_01_126', 'XLU_RET_01_sector_01_126', 'XLV_RET_01_sector_01_126',\n",
    "              'XLY_RET_01_sector_01_126']\n",
    "exp = sb_sector[factor_col]\n",
    "min_exp_weights = {\n",
    "    'XLB_RET_01_sector_01_126': 1,\n",
    "    'XLE_RET_01_sector_01_126': 1,\n",
    "    'XLF_RET_01_sector_01_126': 1,\n",
    "    'XLI_RET_01_sector_01_126': 1,\n",
    "    'XLK_RET_01_sector_01_126': 1,\n",
    "    'XLP_RET_01_sector_01_126': 1,\n",
    "    'XLU_RET_01_sector_01_126': 1,\n",
    "    'XLV_RET_01_sector_01_126': 1,\n",
    "    'XLY_RET_01_sector_01_126': 1\n",
    "}\n",
    "lambda_reg = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a84182-7d05-40ae-8278-7f98f0a3846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred['totalStocks'] = pred.apply(lambda row: row['longStocks'] + row['shortStocks'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b6a201-df7b-446e-95b9-c5cc0011c38a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "long_min_weights = mfl(data=returns, stocks_to_use=pred, col_name='longStocks', exp=exp, min_exp_weights=min_exp_weights, lambda_reg=lambda_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad67da0-bee5-4446-98d2-f510a55049a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "short_min_weights = mfl(data=returns, stocks_to_use=pred, col_name='shortStocks', exp=exp, min_exp_weights=min_exp_weights, lambda_reg=lambda_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b325da-ffdc-479c-bcea-af4580de1f6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_min_weights = mfl(data=returns, stocks_to_use=pred, col_name='totalStocks', exp=exp, min_exp_weights=min_exp_weights, lambda_reg=lambda_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bc0469-7ddd-4e03-b281-7881a2907244",
   "metadata": {},
   "source": [
    "### Calculate Portfolio Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf94983-db27-4ba8-a955-5111ffde320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_exp_weight(data, weight, type):\n",
    "    data = pd.merge(data, weight, left_index=True, right_index=True, how='left')\n",
    "    if type == 'long':\n",
    "        data['total_ret'] = data['returns'] * data['exp_weight'] * 1\n",
    "    elif type == 'short':\n",
    "        data['total_ret'] = data['returns'] * data['exp_weight'] * -1\n",
    "    total_ret = data.groupby('date').total_ret.sum()\n",
    "    return data, total_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabc0e8d-ecc3-44ad-80a6-a93d43d0802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_stock(group):\n",
    "    date = group.index.get_level_values('date')[0]\n",
    "    list = pred.loc[pred.index == date]['longStocks'][0]\n",
    "    return get_stocks_data(group, list)\n",
    "\n",
    "def short_stock(group):\n",
    "    date = group.index.get_level_values('date')[0]\n",
    "    list = pred.loc[pred.index == date]['shortStocks'][0]\n",
    "    return get_stocks_data(group, list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35b861e-1557-4aa9-9066-691f2a5c74ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_stocks = returns.groupby('date').apply(long_stock).reset_index(level=0, drop=True)\n",
    "short_stocks = returns.groupby('date').apply(short_stock).reset_index(level=0, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d3800a-0d64-42f3-ad2b-bc6d0a6ff363",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# long_df, long_ret = set_exp_weight(data=long_stocks[['returns']], weight=long_min_weights, type='long')\n",
    "long_df, long_ret = set_exp_weight(data=long_stocks[['returns']], weight=all_min_weights, type='long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4639fce-b3f0-477c-89a2-ffae425e7d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# short_df, short_ret = set_exp_weight(data=short_stocks[['returns']], weight=short_min_weights, type='short')\n",
    "short_df, short_ret = set_exp_weight(data=short_stocks[['returns']], weight=all_min_weights, type='short')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50958906-3f82-4d25-92ea-8414a42809a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ret = 0.5 * long_ret + 0.5 * short_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fc62d5-cec3-4b12-905c-ba0678740034",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qs.reports.full(total_ret, 'SPY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e046c9-b138-4fe3-9e8a-0685c3a86000",
   "metadata": {},
   "source": [
    "# Inverse Volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb2cfe2-71b7-4714-9d50-c0b3396d336f",
   "metadata": {},
   "source": [
    "### Calculate Portfolio Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13175c44-3a18-4adf-bfea-f39d5b01e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_returns = returns.copy(deep=True)\n",
    "vol_returns['accurate_returns'] = vol_returns.groupby('permno')['returns'].shift(1)\n",
    "vol_returns['vol'] = vol_returns.groupby('permno')['accurate_returns'].transform(lambda x: x.rolling(5).std().shift(1))\n",
    "vol_returns['inv_vol_weight'] = 1 / vol_returns['vol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673c0729-b0de-48af-9db8-b39b462e210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_stocks = vol_returns.groupby('date').apply(long_stock).reset_index(level=0, drop=True)\n",
    "short_stocks = vol_returns.groupby('date').apply(short_stock).reset_index(level=0, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deb74d9-135f-4905-99c1-19fe461e9f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_stocks['type'] = 1\n",
    "short_stocks['type'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbb570e-395c-4b2b-b37b-f30a1df6fdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_stocks = pd.concat([long_stocks, short_stocks], axis=0).sort_index(level=['permno', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359f65c9-651f-4aaf-ae71-dfb73a92afa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normalizing Weights\n",
    "total_stocks['inv_vol_weight'] /= long_stocks.groupby('date')['inv_vol_weight'].transform(lambda x: x.abs().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1850fa-9361-4e5b-8fb2-0e2c0d5da035",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_stocks['total_ret'] = total_stocks['returns'] * total_stocks['inv_vol_weight'] * total_stocks['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e15e16-dbfb-4bb8-a889-72d3dceb5eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ret = total_stocks.groupby('date').total_ret.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab4ee55-9735-4f5e-a5bc-f9a00def621a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qs.reports.full(total_ret, 'SPY')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algofacto",
   "language": "python",
   "name": "algofacto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
